{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the given customer reviews on the products and the reviews of the website about an E – commerce company. We have to predict the rating given by a customer based on the text data written in the review.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E-Business companies have their own review and ratings application in their platform. But often, reviews of a product spread across multiple sources like Facebook posts, Tweets, Blogs etc.  Companies can make use of the data available in their platform to train Machine Learning / Deep Learning models and predict ratings for the reviews available in social media platforms. \n",
    "\n",
    "Business can utilize these ratings to understand the products reach and customer sentiments on a product. It will help them to improve their marketing strategies, build better products, provide good services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "#Configure Visualization Defaults\n",
    "#%matplotlib inline = show plots in Jupyter Notebook browser\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# We will use the following Scikit-Learn classes for the activity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import metrics \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data_train = pd.read_csv(\"./Dataset/train.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>77075.000000</td>\n",
       "      <td>77075.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38538.000000</td>\n",
       "      <td>4.306546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>22249.780336</td>\n",
       "      <td>1.052075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19269.500000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>38538.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>57806.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77075.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID       overall\n",
       "count  77075.000000  77075.000000\n",
       "mean   38538.000000      4.306546\n",
       "std    22249.780336      1.052075\n",
       "min        1.000000      1.000000\n",
       "25%    19269.500000      4.000000\n",
       "50%    38538.000000      5.000000\n",
       "75%    57806.500000      5.000000\n",
       "max    77075.000000      5.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID             int64\n",
       "reviewText    object\n",
       "overall        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data_test = pd.read_csv(\"./Dataset/test.csv\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 77075 entries, 0 to 77074\n",
      "Data columns (total 3 columns):\n",
      "ID            77075 non-null int64\n",
      "reviewText    77049 non-null object\n",
      "overall       77075 non-null int64\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1.8+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61626</th>\n",
       "      <td>61627</td>\n",
       "      <td>Good price but have not used yet.................</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40069</th>\n",
       "      <td>40070</td>\n",
       "      <td>Good program. Interesting angle here. I was re...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76832</th>\n",
       "      <td>76833</td>\n",
       "      <td>I compared this to to peat pellets for rooting...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23987</th>\n",
       "      <td>23988</td>\n",
       "      <td>This is a really amazing portable Midi Keyboar...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10152</th>\n",
       "      <td>10153</td>\n",
       "      <td>We have had live traps for rabbits and garden ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53634</th>\n",
       "      <td>53635</td>\n",
       "      <td>And this show has it all, sci-fi, quantum phys...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73615</th>\n",
       "      <td>73616</td>\n",
       "      <td>love it. bring some tissues, makes me wife cry...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8105</th>\n",
       "      <td>8106</td>\n",
       "      <td>The perlite arrived well packed. I did not hav...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73470</th>\n",
       "      <td>73471</td>\n",
       "      <td>Great story, great acting and the different sh...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44181</th>\n",
       "      <td>44182</td>\n",
       "      <td>Great price and it attracts bugs. It is on the...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                         reviewText  overall\n",
       "61626  61627  Good price but have not used yet.................        4\n",
       "40069  40070  Good program. Interesting angle here. I was re...        4\n",
       "76832  76833  I compared this to to peat pellets for rooting...        4\n",
       "23987  23988  This is a really amazing portable Midi Keyboar...        5\n",
       "10152  10153  We have had live traps for rabbits and garden ...        5\n",
       "53634  53635  And this show has it all, sci-fi, quantum phys...        5\n",
       "73615  73616  love it. bring some tissues, makes me wife cry...        5\n",
       "8105    8106  The perlite arrived well packed. I did not hav...        5\n",
       "73470  73471  Great story, great acting and the different sh...        5\n",
       "44181  44182  Great price and it attracts bugs. It is on the...        5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preview data\n",
    "print (review_data_train.info()) #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html\n",
    "#data_raw.head() #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html\n",
    "#data_raw.tail() #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tail.html\n",
    "review_data_train.sample(10) #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4057 entries, 0 to 4056\n",
      "Data columns (total 2 columns):\n",
      "ID            4057 non-null int64\n",
      "reviewText    4056 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 63.5+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2924</th>\n",
       "      <td>80000</td>\n",
       "      <td>Have not used it yet but come this winter I am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>77258</td>\n",
       "      <td>The banter between Steve and Danno is up to pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>77136</td>\n",
       "      <td>Best guitar picks. Great shape, sturdy. No fan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3663</th>\n",
       "      <td>80739</td>\n",
       "      <td>this season of veronica mars was thriling it a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>78891</td>\n",
       "      <td>This series is addictive for all the right rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3538</th>\n",
       "      <td>80614</td>\n",
       "      <td>We happened upon this show by chance... it was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2124</th>\n",
       "      <td>79200</td>\n",
       "      <td>Guitar picks. What can I say? I bought these a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>77809</td>\n",
       "      <td>Some very strong character development from al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853</th>\n",
       "      <td>78929</td>\n",
       "      <td>Very interesting.  Even though I have watch ot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3884</th>\n",
       "      <td>80960</td>\n",
       "      <td>Alphas is an original, fascinating show. It de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                         reviewText\n",
       "2924  80000  Have not used it yet but come this winter I am...\n",
       "182   77258  The banter between Steve and Danno is up to pa...\n",
       "60    77136  Best guitar picks. Great shape, sturdy. No fan...\n",
       "3663  80739  this season of veronica mars was thriling it a...\n",
       "1815  78891  This series is addictive for all the right rea...\n",
       "3538  80614  We happened upon this show by chance... it was...\n",
       "2124  79200  Guitar picks. What can I say? I bought these a...\n",
       "733   77809  Some very strong character development from al...\n",
       "1853  78929  Very interesting.  Even though I have watch ot...\n",
       "3884  80960  Alphas is an original, fascinating show. It de..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preview data\n",
    "print (review_data_test.info()) #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html\n",
    "#data_raw.head() #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html\n",
    "#data_raw.tail() #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tail.html\n",
    "review_data_test.sample(10) #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>436</td>\n",
       "      <td>I am very pleased with this battery maintainer...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>437</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>438</td>\n",
       "      <td>This is a porous synthetic mat to use for drai...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>439</td>\n",
       "      <td>The first episode of any new series has the un...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>440</td>\n",
       "      <td>Well Sandra Oh is leaving the show--Christina ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>441</td>\n",
       "      <td>This is a great product but could be made a li...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>442</td>\n",
       "      <td>I'll be honest:  I don't really like giving pr...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>443</td>\n",
       "      <td>This trellis netting work great for vertical g...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>444</td>\n",
       "      <td>It's sort of german mythical/fable mixed in wi...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>445</td>\n",
       "      <td>I have to join in the chorus of dismay, even a...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>446</td>\n",
       "      <td>I am using this product for many years. the fi...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>447</td>\n",
       "      <td>I bought my truck used and it was missing the ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>448</td>\n",
       "      <td>And works well for it. My only concern is that...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>449</td>\n",
       "      <td>The second season so far is great. Love the ac...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>450</td>\n",
       "      <td>Replaced used wash cloth. Very good buy. Fits ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>451</td>\n",
       "      <td>I love the show and its main characters.  It i...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>452</td>\n",
       "      <td>Great show, weak season. It was short and loca...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>453</td>\n",
       "      <td>Really, Love to watch Wallace and Gromit episo...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>454</td>\n",
       "      <td>I have some 10mm holes around an access hole i...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>455</td>\n",
       "      <td>It works as advertised but it removes all shin...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>456</td>\n",
       "      <td>I was hesitant to show this to my daughter,sin...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>457</td>\n",
       "      <td>I purchased this for my guitar lessons studio ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>458</td>\n",
       "      <td>See I really liked and got the whole point of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>459</td>\n",
       "      <td>The show is too scripted. A reality show is SU...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>460</td>\n",
       "      <td>The Grit Guard is no gimmick - it definitely w...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>461</td>\n",
       "      <td>Keeping in mind the difference types of guitar...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>462</td>\n",
       "      <td>My cat has caught two mice outside over the pa...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>463</td>\n",
       "      <td>We feed birds, so the yard is loaded with impy...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>464</td>\n",
       "      <td>Have not yet needed to use them, bought them f...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>465</td>\n",
       "      <td>I have thoroughly enjoyed watching the Season ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77045</th>\n",
       "      <td>77046</td>\n",
       "      <td>Great deal for hitch and wiring. The hardest p...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77046</th>\n",
       "      <td>77047</td>\n",
       "      <td>This vent fit perfect in replacing the old one...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77047</th>\n",
       "      <td>77048</td>\n",
       "      <td>I was looking for a micro fiber towel for use ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77048</th>\n",
       "      <td>77049</td>\n",
       "      <td>This series is intended to present a personal ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77049</th>\n",
       "      <td>77050</td>\n",
       "      <td>So, for $30, I wasn't expecting the jack to en...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77050</th>\n",
       "      <td>77051</td>\n",
       "      <td>I put one of these in each car for emergencies...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77051</th>\n",
       "      <td>77052</td>\n",
       "      <td>My 2 year old granddaughter purchased this whe...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77052</th>\n",
       "      <td>77053</td>\n",
       "      <td>Well worth your time if you like drama with a ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77053</th>\n",
       "      <td>77054</td>\n",
       "      <td>I enjoy this program - it is fun to watch even...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77054</th>\n",
       "      <td>77055</td>\n",
       "      <td>I have had this amp for quite some time and fi...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77055</th>\n",
       "      <td>77056</td>\n",
       "      <td>The series continues to be composed of high qu...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77056</th>\n",
       "      <td>77057</td>\n",
       "      <td>Product is as advertised and I would buy again...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77057</th>\n",
       "      <td>77058</td>\n",
       "      <td>A pretty sad movie if you saw it on its own, b...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77058</th>\n",
       "      <td>77059</td>\n",
       "      <td>I was given one blade for free to test out.  I...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77059</th>\n",
       "      <td>77060</td>\n",
       "      <td>Got the Meguiar's Professional Dual Action Pol...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77060</th>\n",
       "      <td>77061</td>\n",
       "      <td>I like the build quality and the look of the g...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77061</th>\n",
       "      <td>77062</td>\n",
       "      <td>For 10-20 dollars you can replace your cabin a...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77062</th>\n",
       "      <td>77063</td>\n",
       "      <td>It was this show that made me realize that the...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77063</th>\n",
       "      <td>77064</td>\n",
       "      <td>My Honda Accord needs one 26\" wiper and one 18...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77064</th>\n",
       "      <td>77065</td>\n",
       "      <td>Cannot say enough good things about this.  Was...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77065</th>\n",
       "      <td>77066</td>\n",
       "      <td>For as long as I can recall, I've never heard ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77066</th>\n",
       "      <td>77067</td>\n",
       "      <td>Good but not as good as season one. Darker cha...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77067</th>\n",
       "      <td>77068</td>\n",
       "      <td>I live in Florida and drive A LOT, we are also...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77068</th>\n",
       "      <td>77069</td>\n",
       "      <td>I've tried several of the various mouse contro...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77069</th>\n",
       "      <td>77070</td>\n",
       "      <td>This is the direct replacement part for the B&amp;...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77070</th>\n",
       "      <td>77071</td>\n",
       "      <td>Started watching this and got hooked on the ch...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77071</th>\n",
       "      <td>77072</td>\n",
       "      <td>I put a new battery in my truck and I usually ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77072</th>\n",
       "      <td>77073</td>\n",
       "      <td>I love this pedal! That said I would like to a...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77073</th>\n",
       "      <td>77074</td>\n",
       "      <td>When I first watched the pilot, I saw too many...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77074</th>\n",
       "      <td>77075</td>\n",
       "      <td>This show just gets better and better, the sto...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76640 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                         reviewText  overall\n",
       "435      436  I am very pleased with this battery maintainer...        5\n",
       "436      437                                                NaN        4\n",
       "437      438  This is a porous synthetic mat to use for drai...        5\n",
       "438      439  The first episode of any new series has the un...        1\n",
       "439      440  Well Sandra Oh is leaving the show--Christina ...        5\n",
       "440      441  This is a great product but could be made a li...        3\n",
       "441      442  I'll be honest:  I don't really like giving pr...        4\n",
       "442      443  This trellis netting work great for vertical g...        4\n",
       "443      444  It's sort of german mythical/fable mixed in wi...        4\n",
       "444      445  I have to join in the chorus of dismay, even a...        3\n",
       "445      446  I am using this product for many years. the fi...        5\n",
       "446      447  I bought my truck used and it was missing the ...        5\n",
       "447      448  And works well for it. My only concern is that...        5\n",
       "448      449  The second season so far is great. Love the ac...        5\n",
       "449      450  Replaced used wash cloth. Very good buy. Fits ...        5\n",
       "450      451  I love the show and its main characters.  It i...        5\n",
       "451      452  Great show, weak season. It was short and loca...        3\n",
       "452      453  Really, Love to watch Wallace and Gromit episo...        5\n",
       "453      454  I have some 10mm holes around an access hole i...        5\n",
       "454      455  It works as advertised but it removes all shin...        3\n",
       "455      456  I was hesitant to show this to my daughter,sin...        5\n",
       "456      457  I purchased this for my guitar lessons studio ...        5\n",
       "457      458  See I really liked and got the whole point of ...        1\n",
       "458      459  The show is too scripted. A reality show is SU...        2\n",
       "459      460  The Grit Guard is no gimmick - it definitely w...        5\n",
       "460      461  Keeping in mind the difference types of guitar...        4\n",
       "461      462  My cat has caught two mice outside over the pa...        5\n",
       "462      463  We feed birds, so the yard is loaded with impy...        4\n",
       "463      464  Have not yet needed to use them, bought them f...        5\n",
       "464      465  I have thoroughly enjoyed watching the Season ...        5\n",
       "...      ...                                                ...      ...\n",
       "77045  77046  Great deal for hitch and wiring. The hardest p...        5\n",
       "77046  77047  This vent fit perfect in replacing the old one...        5\n",
       "77047  77048  I was looking for a micro fiber towel for use ...        3\n",
       "77048  77049  This series is intended to present a personal ...        3\n",
       "77049  77050  So, for $30, I wasn't expecting the jack to en...        4\n",
       "77050  77051  I put one of these in each car for emergencies...        4\n",
       "77051  77052  My 2 year old granddaughter purchased this whe...        5\n",
       "77052  77053  Well worth your time if you like drama with a ...        5\n",
       "77053  77054  I enjoy this program - it is fun to watch even...        4\n",
       "77054  77055  I have had this amp for quite some time and fi...        5\n",
       "77055  77056  The series continues to be composed of high qu...        5\n",
       "77056  77057  Product is as advertised and I would buy again...        5\n",
       "77057  77058  A pretty sad movie if you saw it on its own, b...        4\n",
       "77058  77059  I was given one blade for free to test out.  I...        4\n",
       "77059  77060  Got the Meguiar's Professional Dual Action Pol...        5\n",
       "77060  77061  I like the build quality and the look of the g...        4\n",
       "77061  77062  For 10-20 dollars you can replace your cabin a...        4\n",
       "77062  77063  It was this show that made me realize that the...        5\n",
       "77063  77064  My Honda Accord needs one 26\" wiper and one 18...        3\n",
       "77064  77065  Cannot say enough good things about this.  Was...        5\n",
       "77065  77066  For as long as I can recall, I've never heard ...        5\n",
       "77066  77067  Good but not as good as season one. Darker cha...        4\n",
       "77067  77068  I live in Florida and drive A LOT, we are also...        5\n",
       "77068  77069  I've tried several of the various mouse contro...        4\n",
       "77069  77070  This is the direct replacement part for the B&...        5\n",
       "77070  77071  Started watching this and got hooked on the ch...        5\n",
       "77071  77072  I put a new battery in my truck and I usually ...        4\n",
       "77072  77073  I love this pedal! That said I would like to a...        5\n",
       "77073  77074  When I first watched the pilot, I saw too many...        5\n",
       "77074  77075  This show just gets better and better, the sto...        5\n",
       "\n",
       "[76640 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_train.iloc[435:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>437</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>481</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>1335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>1937</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4425</th>\n",
       "      <td>4426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9959</th>\n",
       "      <td>9960</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11108</th>\n",
       "      <td>11109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11801</th>\n",
       "      <td>11802</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11959</th>\n",
       "      <td>11960</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15311</th>\n",
       "      <td>15312</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28717</th>\n",
       "      <td>28718</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31343</th>\n",
       "      <td>31344</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31643</th>\n",
       "      <td>31644</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35109</th>\n",
       "      <td>35110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36549</th>\n",
       "      <td>36550</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37028</th>\n",
       "      <td>37029</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45390</th>\n",
       "      <td>45391</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46300</th>\n",
       "      <td>46301</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59347</th>\n",
       "      <td>59348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60971</th>\n",
       "      <td>60972</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62004</th>\n",
       "      <td>62005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62069</th>\n",
       "      <td>62070</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62837</th>\n",
       "      <td>62838</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64092</th>\n",
       "      <td>64093</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72842</th>\n",
       "      <td>72843</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75695</th>\n",
       "      <td>75696</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID reviewText  overall\n",
       "436      437        NaN        4\n",
       "480      481        NaN        4\n",
       "1334    1335        NaN        5\n",
       "1936    1937        NaN        5\n",
       "4425    4426        NaN        3\n",
       "9959    9960        NaN        2\n",
       "11108  11109        NaN        1\n",
       "11801  11802        NaN        5\n",
       "11959  11960        NaN        5\n",
       "15311  15312        NaN        5\n",
       "28717  28718        NaN        5\n",
       "31343  31344        NaN        1\n",
       "31643  31644        NaN        5\n",
       "35109  35110        NaN        5\n",
       "36549  36550        NaN        5\n",
       "37028  37029        NaN        5\n",
       "45390  45391        NaN        4\n",
       "46300  46301        NaN        4\n",
       "59347  59348        NaN        5\n",
       "60971  60972        NaN        5\n",
       "62004  62005        NaN        5\n",
       "62069  62070        NaN        3\n",
       "62837  62838        NaN        5\n",
       "64092  64093        NaN        4\n",
       "72842  72843        NaN        5\n",
       "75695  75696        NaN        5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_train[review_data_train['reviewText'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns with null values:\n",
      " ID             0\n",
      "reviewText    26\n",
      "overall        0\n",
      "dtype: int64\n",
      "----------\n",
      "Test/Validation columns with null values:\n",
      " ID            0\n",
      "reviewText    1\n",
      "dtype: int64\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>77075.000000</td>\n",
       "      <td>77049</td>\n",
       "      <td>77075.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>77033</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>#NAME?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38538.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.306546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>22249.780336</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.052075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19269.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>38538.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>57806.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77075.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ID reviewText       overall\n",
       "count   77075.000000      77049  77075.000000\n",
       "unique           NaN      77033           NaN\n",
       "top              NaN     #NAME?           NaN\n",
       "freq             NaN          3           NaN\n",
       "mean    38538.000000        NaN      4.306546\n",
       "std     22249.780336        NaN      1.052075\n",
       "min         1.000000        NaN      1.000000\n",
       "25%     19269.500000        NaN      4.000000\n",
       "50%     38538.000000        NaN      5.000000\n",
       "75%     57806.500000        NaN      5.000000\n",
       "max     77075.000000        NaN      5.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train columns with null values:\\n', review_data_train.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print('Test/Validation columns with null values:\\n', review_data_test.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "review_data_train.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>These are so wonderful to have in the car in t...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>XL fits perfect on me over armored riding jack...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Since I had just noticed my wiper blades neede...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>I am very satisfied with G110v2.  It is comfor...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>This amp did a good job for its rating. It was...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                         reviewText  overall\n",
       "0   1  These are so wonderful to have in the car in t...        5\n",
       "1   2  XL fits perfect on me over armored riding jack...        5\n",
       "2   3  Since I had just noticed my wiper blades neede...        3\n",
       "3   4  I am very satisfied with G110v2.  It is comfor...        5\n",
       "4   5  This amp did a good job for its rating. It was...        3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>These are so wonderful to have in the car in t...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>XL fits perfect on me over armored riding jack...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Since I had just noticed my wiper blades neede...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>I am very satisfied with G110v2.  It is comfor...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>This amp did a good job for its rating. It was...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                         reviewText  overall\n",
       "0   1  These are so wonderful to have in the car in t...        5\n",
       "1   2  XL fits perfect on me over armored riding jack...        5\n",
       "2   3  Since I had just noticed my wiper blades neede...        3\n",
       "3   4  I am very satisfied with G110v2.  It is comfor...        5\n",
       "4   5  This amp did a good job for its rating. It was...        3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x106371588>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "review_data_train[\"overall\"].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are 46317 values that are giving 5 rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    46317\n",
       "4    16997\n",
       "3     7676\n",
       "2     3241\n",
       "1     2844\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_train[\"overall\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_re = re.compile('[^a-z]+')\n",
    "\n",
    "def cleanup(sentence):\n",
    "    sentence = str(sentence)\n",
    "    sentence = re.sub(r'\\s+',' ',sentence) #\\s is for white spaces\n",
    "    sentence = re.sub('[\\d]','',sentence)  #\\d is for digits\n",
    "    sentence = sentence.lower()\n",
    "    sentence = cleanup_re.sub(' ', sentence).strip()\n",
    "    #sentence = \" \".join(nltk.word_tokenize(sentence))\n",
    "    return sentence\n",
    "\n",
    "\n",
    "review_data_train[\"Summary_Clean_reviewtext\"] = review_data_train[\"reviewText\"].apply(cleanup)\n",
    "review_data_test[\"Summary_Clean_reviewtext\"] = review_data_test[\"reviewText\"].apply(cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>Summary_Clean_reviewtext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>These are so wonderful to have in the car in t...</td>\n",
       "      <td>5</td>\n",
       "      <td>these are so wonderful to have in the car in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>XL fits perfect on me over armored riding jack...</td>\n",
       "      <td>5</td>\n",
       "      <td>xl fits perfect on me over armored riding jack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Since I had just noticed my wiper blades neede...</td>\n",
       "      <td>3</td>\n",
       "      <td>since i had just noticed my wiper blades neede...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>I am very satisfied with G110v2.  It is comfor...</td>\n",
       "      <td>5</td>\n",
       "      <td>i am very satisfied with gv it is comfortable ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>This amp did a good job for its rating. It was...</td>\n",
       "      <td>3</td>\n",
       "      <td>this amp did a good job for its rating it was ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                         reviewText  overall  \\\n",
       "0   1  These are so wonderful to have in the car in t...        5   \n",
       "1   2  XL fits perfect on me over armored riding jack...        5   \n",
       "2   3  Since I had just noticed my wiper blades neede...        3   \n",
       "3   4  I am very satisfied with G110v2.  It is comfor...        5   \n",
       "4   5  This amp did a good job for its rating. It was...        3   \n",
       "\n",
       "                            Summary_Clean_reviewtext  \n",
       "0  these are so wonderful to have in the car in t...  \n",
       "1  xl fits perfect on me over armored riding jack...  \n",
       "2  since i had just noticed my wiper blades neede...  \n",
       "3  i am very satisfied with gv it is comfortable ...  \n",
       "4  this amp did a good job for its rating it was ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>Summary_Clean_reviewtext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>These are so wonderful to have in the car in t...</td>\n",
       "      <td>5</td>\n",
       "      <td>these are so wonderful to have in the car in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>XL fits perfect on me over armored riding jack...</td>\n",
       "      <td>5</td>\n",
       "      <td>xl fits perfect on me over armored riding jack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Since I had just noticed my wiper blades neede...</td>\n",
       "      <td>3</td>\n",
       "      <td>since i had just noticed my wiper blades neede...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>I am very satisfied with G110v2.  It is comfor...</td>\n",
       "      <td>5</td>\n",
       "      <td>i am very satisfied with gv it is comfortable ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>This amp did a good job for its rating. It was...</td>\n",
       "      <td>3</td>\n",
       "      <td>this amp did a good job for its rating it was ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                         reviewText  overall  \\\n",
       "0   1  These are so wonderful to have in the car in t...        5   \n",
       "1   2  XL fits perfect on me over armored riding jack...        5   \n",
       "2   3  Since I had just noticed my wiper blades neede...        3   \n",
       "3   4  I am very satisfied with G110v2.  It is comfor...        5   \n",
       "4   5  This amp did a good job for its rating. It was...        3   \n",
       "\n",
       "                            Summary_Clean_reviewtext  \n",
       "0  these are so wonderful to have in the car in t...  \n",
       "1  xl fits perfect on me over armored riding jack...  \n",
       "2  since i had just noticed my wiper blades neede...  \n",
       "3  i am very satisfied with gv it is comfortable ...  \n",
       "4  this amp did a good job for its rating it was ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>Summary_Clean_reviewtext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77076</td>\n",
       "      <td>My idea of &amp;#34;gardening&amp;#34; is to have lots...</td>\n",
       "      <td>my idea of gardening is to have lots of hangin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77077</td>\n",
       "      <td>It's actually sleek looking and although I did...</td>\n",
       "      <td>it s actually sleek looking and although i did...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77078</td>\n",
       "      <td>I'm very claustrophobic, so I couldn't help bu...</td>\n",
       "      <td>i m very claustrophobic so i couldn t help but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77079</td>\n",
       "      <td>The beans I purchased and planted were destroy...</td>\n",
       "      <td>the beans i purchased and planted were destroy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>77080</td>\n",
       "      <td>This is a very amusing comedy great mix of pol...</td>\n",
       "      <td>this is a very amusing comedy great mix of pol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                         reviewText  \\\n",
       "0  77076  My idea of &#34;gardening&#34; is to have lots...   \n",
       "1  77077  It's actually sleek looking and although I did...   \n",
       "2  77078  I'm very claustrophobic, so I couldn't help bu...   \n",
       "3  77079  The beans I purchased and planted were destroy...   \n",
       "4  77080  This is a very amusing comedy great mix of pol...   \n",
       "\n",
       "                            Summary_Clean_reviewtext  \n",
       "0  my idea of gardening is to have lots of hangin...  \n",
       "1  it s actually sleek looking and although i did...  \n",
       "2  i m very claustrophobic so i couldn t help but...  \n",
       "3  the beans i purchased and planted were destroy...  \n",
       "4  this is a very amusing comedy great mix of pol...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>Summary_Clean_reviewtext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77070</th>\n",
       "      <td>77071</td>\n",
       "      <td>Started watching this and got hooked on the ch...</td>\n",
       "      <td>5</td>\n",
       "      <td>started watching this and got hooked on the ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77071</th>\n",
       "      <td>77072</td>\n",
       "      <td>I put a new battery in my truck and I usually ...</td>\n",
       "      <td>4</td>\n",
       "      <td>i put a new battery in my truck and i usually ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77072</th>\n",
       "      <td>77073</td>\n",
       "      <td>I love this pedal! That said I would like to a...</td>\n",
       "      <td>5</td>\n",
       "      <td>i love this pedal that said i would like to ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77073</th>\n",
       "      <td>77074</td>\n",
       "      <td>When I first watched the pilot, I saw too many...</td>\n",
       "      <td>5</td>\n",
       "      <td>when i first watched the pilot i saw too many ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77074</th>\n",
       "      <td>77075</td>\n",
       "      <td>This show just gets better and better, the sto...</td>\n",
       "      <td>5</td>\n",
       "      <td>this show just gets better and better the stor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                         reviewText  overall  \\\n",
       "77070  77071  Started watching this and got hooked on the ch...        5   \n",
       "77071  77072  I put a new battery in my truck and I usually ...        4   \n",
       "77072  77073  I love this pedal! That said I would like to a...        5   \n",
       "77073  77074  When I first watched the pilot, I saw too many...        5   \n",
       "77074  77075  This show just gets better and better, the sto...        5   \n",
       "\n",
       "                                Summary_Clean_reviewtext  \n",
       "77070  started watching this and got hooked on the ch...  \n",
       "77071  i put a new battery in my truck and i usually ...  \n",
       "77072  i love this pedal that said i would like to ad...  \n",
       "77073  when i first watched the pilot i saw too many ...  \n",
       "77074  this show just gets better and better the stor...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>Summary_Clean_reviewtext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4052</th>\n",
       "      <td>81128</td>\n",
       "      <td>This is as much a reply to the guy who wants t...</td>\n",
       "      <td>this is as much a reply to the guy who wants t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4053</th>\n",
       "      <td>81129</td>\n",
       "      <td>I had the chance to try these out. I have had ...</td>\n",
       "      <td>i had the chance to try these out i have had a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4054</th>\n",
       "      <td>81130</td>\n",
       "      <td>This program is full of wonderfully absurd pos...</td>\n",
       "      <td>this program is full of wonderfully absurd pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4055</th>\n",
       "      <td>81131</td>\n",
       "      <td>I love this show. Always makes me laugh. I nee...</td>\n",
       "      <td>i love this show always makes me laugh i need ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4056</th>\n",
       "      <td>81132</td>\n",
       "      <td>Well, Iike the feeder, easy to clean and hang,...</td>\n",
       "      <td>well iike the feeder easy to clean and hang bu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                         reviewText  \\\n",
       "4052  81128  This is as much a reply to the guy who wants t...   \n",
       "4053  81129  I had the chance to try these out. I have had ...   \n",
       "4054  81130  This program is full of wonderfully absurd pos...   \n",
       "4055  81131  I love this show. Always makes me laugh. I nee...   \n",
       "4056  81132  Well, Iike the feeder, easy to clean and hang,...   \n",
       "\n",
       "                               Summary_Clean_reviewtext  \n",
       "4052  this is as much a reply to the guy who wants t...  \n",
       "4053  i had the chance to try these out i have had a...  \n",
       "4054  this program is full of wonderfully absurd pos...  \n",
       "4055  i love this show always makes me laugh i need ...  \n",
       "4056  well iike the feeder easy to clean and hang bu...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review_data_train = review_data_train.drop(['reviewText'],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#review_data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review_data_test = review_data_test.drop(['reviewText'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review_data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns with null values:\n",
      " ID                           0\n",
      "reviewText                  26\n",
      "overall                      0\n",
      "Summary_Clean_reviewtext     0\n",
      "dtype: int64\n",
      "----------\n",
      "Test/Validation columns with null values:\n",
      " ID                          0\n",
      "reviewText                  1\n",
      "Summary_Clean_reviewtext    0\n",
      "dtype: int64\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>Summary_Clean_reviewtext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>77075.000000</td>\n",
       "      <td>77049</td>\n",
       "      <td>77075.000000</td>\n",
       "      <td>77075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>77033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>#NAME?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38538.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.306546</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>22249.780336</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.052075</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19269.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>38538.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>57806.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77075.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ID reviewText       overall Summary_Clean_reviewtext\n",
       "count   77075.000000      77049  77075.000000                    77075\n",
       "unique           NaN      77033           NaN                    76957\n",
       "top              NaN     #NAME?           NaN                      nan\n",
       "freq             NaN          3           NaN                       26\n",
       "mean    38538.000000        NaN      4.306546                      NaN\n",
       "std     22249.780336        NaN      1.052075                      NaN\n",
       "min         1.000000        NaN      1.000000                      NaN\n",
       "25%     19269.500000        NaN      4.000000                      NaN\n",
       "50%     38538.000000        NaN      5.000000                      NaN\n",
       "75%     57806.500000        NaN      5.000000                      NaN\n",
       "max     77075.000000        NaN      5.000000                      NaN"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train columns with null values:\\n', review_data_train.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print('Test/Validation columns with null values:\\n', review_data_test.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "review_data_train.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing method 1 (Done in a different way than taught in class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use text data to extract a number of features even if we don’t have sufficient knowledge of Natural Language Processing. So let’s discuss some of them in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most basic features we can extract is the number of words in each tweet. The basic intuition behind this is that generally, the negative sentiments contain a lesser amount of words than the positive ones.\n",
    "\n",
    "To do this, we simply use the split function in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary_Clean_reviewtext</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>these are so wonderful to have in the car in t...</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xl fits perfect on me over armored riding jack...</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>since i had just noticed my wiper blades neede...</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am very satisfied with gv it is comfortable ...</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this amp did a good job for its rating it was ...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Summary_Clean_reviewtext  word_count\n",
       "0  these are so wonderful to have in the car in t...         159\n",
       "1  xl fits perfect on me over armored riding jack...          67\n",
       "2  since i had just noticed my wiper blades neede...         354\n",
       "3  i am very satisfied with gv it is comfortable ...         211\n",
       "4  this amp did a good job for its rating it was ...          30"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_train['word_count'] = review_data_train['Summary_Clean_reviewtext'].apply(lambda x: len(str(x).split(\" \")))\n",
    "review_data_train[['Summary_Clean_reviewtext', 'word_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary_Clean_reviewtext</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my idea of gardening is to have lots of hangin...</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it s actually sleek looking and although i did...</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i m very claustrophobic so i couldn t help but...</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the beans i purchased and planted were destroy...</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this is a very amusing comedy great mix of pol...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Summary_Clean_reviewtext  word_count\n",
       "0  my idea of gardening is to have lots of hangin...         125\n",
       "1  it s actually sleek looking and although i did...          88\n",
       "2  i m very claustrophobic so i couldn t help but...         142\n",
       "3  the beans i purchased and planted were destroy...          55\n",
       "4  this is a very amusing comedy great mix of pol...          40"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_test['word_count'] = review_data_test['Summary_Clean_reviewtext'].apply(lambda x: len(str(x).split(\" \")))\n",
    "review_data_test[['Summary_Clean_reviewtext', 'word_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature is also based on the previous feature intuition. Here, we calculate the number of characters in each tweet. This is done by calculating the length of the review text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary_Clean_reviewtext</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>these are so wonderful to have in the car in t...</td>\n",
       "      <td>788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xl fits perfect on me over armored riding jack...</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>since i had just noticed my wiper blades neede...</td>\n",
       "      <td>1919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am very satisfied with gv it is comfortable ...</td>\n",
       "      <td>1127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this amp did a good job for its rating it was ...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Summary_Clean_reviewtext  char_count\n",
       "0  these are so wonderful to have in the car in t...         788\n",
       "1  xl fits perfect on me over armored riding jack...         342\n",
       "2  since i had just noticed my wiper blades neede...        1919\n",
       "3  i am very satisfied with gv it is comfortable ...        1127\n",
       "4  this amp did a good job for its rating it was ...         120"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_train['char_count'] = review_data_train['Summary_Clean_reviewtext'].str.len() ## this also includes spaces\n",
    "review_data_train[['Summary_Clean_reviewtext','char_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary_Clean_reviewtext</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my idea of gardening is to have lots of hangin...</td>\n",
       "      <td>632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it s actually sleek looking and although i did...</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i m very claustrophobic so i couldn t help but...</td>\n",
       "      <td>688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the beans i purchased and planted were destroy...</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this is a very amusing comedy great mix of pol...</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Summary_Clean_reviewtext  char_count\n",
       "0  my idea of gardening is to have lots of hangin...         632\n",
       "1  it s actually sleek looking and although i did...         394\n",
       "2  i m very claustrophobic so i couldn t help but...         688\n",
       "3  the beans i purchased and planted were destroy...         279\n",
       "4  this is a very amusing comedy great mix of pol...         224"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_test['char_count'] = review_data_test['Summary_Clean_reviewtext'].str.len() ## this also includes spaces\n",
    "review_data_test[['Summary_Clean_reviewtext','char_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the calculation will also include the number of spaces, which you can remove, if required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Word Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also extract another feature which will calculate the average word length of each tweet. This can also potentially help us in improving our model.\n",
    "\n",
    "Here, we simply take the sum of the length of all the words and divide it by the total length of the review text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary_Clean_reviewtext</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>these are so wonderful to have in the car in t...</td>\n",
       "      <td>3.962264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xl fits perfect on me over armored riding jack...</td>\n",
       "      <td>4.119403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>since i had just noticed my wiper blades neede...</td>\n",
       "      <td>4.423729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am very satisfied with gv it is comfortable ...</td>\n",
       "      <td>4.345972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this amp did a good job for its rating it was ...</td>\n",
       "      <td>3.033333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Summary_Clean_reviewtext  avg_word\n",
       "0  these are so wonderful to have in the car in t...  3.962264\n",
       "1  xl fits perfect on me over armored riding jack...  4.119403\n",
       "2  since i had just noticed my wiper blades neede...  4.423729\n",
       "3  i am very satisfied with gv it is comfortable ...  4.345972\n",
       "4  this amp did a good job for its rating it was ...  3.033333"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "review_data_train['avg_word'] = review_data_train['Summary_Clean_reviewtext'].apply(lambda x: avg_word(x))\n",
    "review_data_train[['Summary_Clean_reviewtext','avg_word']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary_Clean_reviewtext</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my idea of gardening is to have lots of hangin...</td>\n",
       "      <td>4.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it s actually sleek looking and although i did...</td>\n",
       "      <td>3.488636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i m very claustrophobic so i couldn t help but...</td>\n",
       "      <td>3.852113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the beans i purchased and planted were destroy...</td>\n",
       "      <td>4.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this is a very amusing comedy great mix of pol...</td>\n",
       "      <td>4.625000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Summary_Clean_reviewtext  avg_word\n",
       "0  my idea of gardening is to have lots of hangin...  4.064000\n",
       "1  it s actually sleek looking and although i did...  3.488636\n",
       "2  i m very claustrophobic so i couldn t help but...  3.852113\n",
       "3  the beans i purchased and planted were destroy...  4.090909\n",
       "4  this is a very amusing comedy great mix of pol...  4.625000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_test['avg_word'] = review_data_test['Summary_Clean_reviewtext'].apply(lambda x: avg_word(x))\n",
    "review_data_test[['Summary_Clean_reviewtext','avg_word']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, while solving an NLP problem, the first thing we do is to remove the stopwords. But sometimes calculating the number of stopwords can also give us some extra information which we might have been losing before.\n",
    "\n",
    "Here, we have imported stopwords from NLTK, which is a basic NLP library in python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary_Clean_reviewtext</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>these are so wonderful to have in the car in t...</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xl fits perfect on me over armored riding jack...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>since i had just noticed my wiper blades neede...</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am very satisfied with gv it is comfortable ...</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this amp did a good job for its rating it was ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Summary_Clean_reviewtext  stopwords\n",
       "0  these are so wonderful to have in the car in t...         96\n",
       "1  xl fits perfect on me over armored riding jack...         30\n",
       "2  since i had just noticed my wiper blades neede...        181\n",
       "3  i am very satisfied with gv it is comfortable ...         96\n",
       "4  this amp did a good job for its rating it was ...         19"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "review_data_train['stopwords'] = review_data_train['Summary_Clean_reviewtext'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "review_data_train[['Summary_Clean_reviewtext','stopwords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary_Clean_reviewtext</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my idea of gardening is to have lots of hangin...</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it s actually sleek looking and although i did...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i m very claustrophobic so i couldn t help but...</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the beans i purchased and planted were destroy...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this is a very amusing comedy great mix of pol...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Summary_Clean_reviewtext  stopwords\n",
       "0  my idea of gardening is to have lots of hangin...         69\n",
       "1  it s actually sleek looking and although i did...         52\n",
       "2  i m very claustrophobic so i couldn t help but...         79\n",
       "3  the beans i purchased and planted were destroy...         32\n",
       "4  this is a very amusing comedy great mix of pol...         18"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_test['stopwords'] = review_data_test['Summary_Clean_reviewtext'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "review_data_test[['Summary_Clean_reviewtext','stopwords']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more interesting feature which we can extract from a tweet is calculating the number of hashtags or mentions present in it. This also helps in extracting extra information from our text data.\n",
    "\n",
    "Here, we make use of the ‘starts with’ function because hashtags (or mentions) always appear at the beginning of a word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary_Clean_reviewtext</th>\n",
       "      <th>hastags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>these are so wonderful to have in the car in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xl fits perfect on me over armored riding jack...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>since i had just noticed my wiper blades neede...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am very satisfied with gv it is comfortable ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this amp did a good job for its rating it was ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Summary_Clean_reviewtext  hastags\n",
       "0  these are so wonderful to have in the car in t...        0\n",
       "1  xl fits perfect on me over armored riding jack...        0\n",
       "2  since i had just noticed my wiper blades neede...        0\n",
       "3  i am very satisfied with gv it is comfortable ...        0\n",
       "4  this amp did a good job for its rating it was ...        0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_train['hastags'] = review_data_train['Summary_Clean_reviewtext'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "review_data_train[['Summary_Clean_reviewtext','hastags']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary_Clean_reviewtext</th>\n",
       "      <th>hastags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my idea of gardening is to have lots of hangin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it s actually sleek looking and although i did...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i m very claustrophobic so i couldn t help but...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the beans i purchased and planted were destroy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this is a very amusing comedy great mix of pol...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Summary_Clean_reviewtext  hastags\n",
       "0  my idea of gardening is to have lots of hangin...        0\n",
       "1  it s actually sleek looking and although i did...        0\n",
       "2  i m very claustrophobic so i couldn t help but...        0\n",
       "3  the beans i purchased and planted were destroy...        0\n",
       "4  this is a very amusing comedy great mix of pol...        0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_test['hastags'] = review_data_test['Summary_Clean_reviewtext'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "review_data_test[['Summary_Clean_reviewtext','hastags']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of numerics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we calculated the number of words, we can also calculate the number of numerics which are present in the tweets. It does not have a lot of use in our example, but this is still a useful feature that should be run while doing similar exercises. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary_Clean_reviewtext</th>\n",
       "      <th>numerics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>these are so wonderful to have in the car in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xl fits perfect on me over armored riding jack...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>since i had just noticed my wiper blades neede...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am very satisfied with gv it is comfortable ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this amp did a good job for its rating it was ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Summary_Clean_reviewtext  numerics\n",
       "0  these are so wonderful to have in the car in t...         0\n",
       "1  xl fits perfect on me over armored riding jack...         0\n",
       "2  since i had just noticed my wiper blades neede...         0\n",
       "3  i am very satisfied with gv it is comfortable ...         0\n",
       "4  this amp did a good job for its rating it was ...         0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_train['numerics'] = review_data_train['Summary_Clean_reviewtext'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "review_data_train[['Summary_Clean_reviewtext','numerics']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary_Clean_reviewtext</th>\n",
       "      <th>numerics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my idea of gardening is to have lots of hangin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it s actually sleek looking and although i did...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i m very claustrophobic so i couldn t help but...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the beans i purchased and planted were destroy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this is a very amusing comedy great mix of pol...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Summary_Clean_reviewtext  numerics\n",
       "0  my idea of gardening is to have lots of hangin...         0\n",
       "1  it s actually sleek looking and although i did...         0\n",
       "2  i m very claustrophobic so i couldn t help but...         0\n",
       "3  the beans i purchased and planted were destroy...         0\n",
       "4  this is a very amusing comedy great mix of pol...         0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_test['numerics'] = review_data_test['Summary_Clean_reviewtext'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "review_data_test[['Summary_Clean_reviewtext','numerics']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Uppercase words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anger or rage is quite often expressed by writing in UPPERCASE words which makes this a necessary operation to identify those words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary_Clean_reviewtext</th>\n",
       "      <th>upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>these are so wonderful to have in the car in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xl fits perfect on me over armored riding jack...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>since i had just noticed my wiper blades neede...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am very satisfied with gv it is comfortable ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this amp did a good job for its rating it was ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Summary_Clean_reviewtext  upper\n",
       "0  these are so wonderful to have in the car in t...      0\n",
       "1  xl fits perfect on me over armored riding jack...      0\n",
       "2  since i had just noticed my wiper blades neede...      0\n",
       "3  i am very satisfied with gv it is comfortable ...      0\n",
       "4  this amp did a good job for its rating it was ...      0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_train['upper'] = review_data_train['Summary_Clean_reviewtext'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "review_data_train[['Summary_Clean_reviewtext','upper']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary_Clean_reviewtext</th>\n",
       "      <th>upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my idea of gardening is to have lots of hangin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it s actually sleek looking and although i did...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i m very claustrophobic so i couldn t help but...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the beans i purchased and planted were destroy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this is a very amusing comedy great mix of pol...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Summary_Clean_reviewtext  upper\n",
       "0  my idea of gardening is to have lots of hangin...      0\n",
       "1  it s actually sleek looking and although i did...      0\n",
       "2  i m very claustrophobic so i couldn t help but...      0\n",
       "3  the beans i purchased and planted were destroy...      0\n",
       "4  this is a very amusing comedy great mix of pol...      0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_test['upper'] = review_data_test['Summary_Clean_reviewtext'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "review_data_test[['Summary_Clean_reviewtext','upper']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first pre-processing step which we will do is transform our tweets into lower case. This avoids having multiple copies of the same words. For example, while calculating the word count, ‘Analytics’ and ‘analytics’ will be taken as different words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    these are so wonderful to have in the car in t...\n",
       "1    xl fits perfect on me over armored riding jack...\n",
       "2    since i had just noticed my wiper blades neede...\n",
       "3    i am very satisfied with gv it is comfortable ...\n",
       "4    this amp did a good job for its rating it was ...\n",
       "Name: Clean, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_train['Clean'] = review_data_train['Summary_Clean_reviewtext'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "review_data_train['Clean'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    my idea of gardening is to have lots of hangin...\n",
       "1    it s actually sleek looking and although i did...\n",
       "2    i m very claustrophobic so i couldn t help but...\n",
       "3    the beans i purchased and planted were destroy...\n",
       "4    this is a very amusing comedy great mix of pol...\n",
       "Name: Clean, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_test['Clean'] = review_data_test['Summary_Clean_reviewtext'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "review_data_test['Clean'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to remove punctuation, as it doesn’t add any extra information while treating text data. Therefore removing all instances of it will help us reduce the size of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    these are so wonderful to have in the car in t...\n",
       "1    xl fits perfect on me over armored riding jack...\n",
       "2    since i had just noticed my wiper blades neede...\n",
       "3    i am very satisfied with gv it is comfortable ...\n",
       "4    this amp did a good job for its rating it was ...\n",
       "Name: Clean, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_train['Clean'] = review_data_train['Summary_Clean_reviewtext'].str.replace('[^\\w\\s]','')\n",
    "review_data_train['Clean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    my idea of gardening is to have lots of hangin...\n",
       "1    it s actually sleek looking and although i did...\n",
       "2    i m very claustrophobic so i couldn t help but...\n",
       "3    the beans i purchased and planted were destroy...\n",
       "4    this is a very amusing comedy great mix of pol...\n",
       "Name: Clean, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_test['Clean'] = review_data_test['Summary_Clean_reviewtext'].str.replace('[^\\w\\s]','')\n",
    "review_data_test['Clean'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the above output, all the punctuation, including ‘#’ and ‘@’, has been removed from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed earlier, stop words (or commonly occurring words) should be removed from the text data. For this purpose, we can either create a list of stopwords ourselves or we can use predefined libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    wonderful car winter getting one another color...\n",
       "1    xl fits perfect armored riding jacket lbs rush...\n",
       "2    since noticed wiper blades needed replacing pl...\n",
       "3    satisfied gv comfortable hold easy use vibrate...\n",
       "4    amp good job rating backup amp qsc gx sold usi...\n",
       "Name: Clean, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "review_data_train['Clean'] = review_data_train['Summary_Clean_reviewtext'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "review_data_train['Clean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    idea gardening lots hanging baskets container ...\n",
       "1    actually sleek looking although nerve actually...\n",
       "2    claustrophobic help imagine react stuck mudsli...\n",
       "3    beans purchased planted destroyed root opossum...\n",
       "4    amusing comedy great mix politics court room d...\n",
       "Name: Clean, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_test['Clean'] = review_data_test['Summary_Clean_reviewtext'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "review_data_test['Clean'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common word removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we just removed commonly occurring words in a general sense. We can also remove commonly occurring words from our text data First, let’s check the 10 most frequently occurring words in our text data then take call to remove or retain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the     426837\n",
       "and     228231\n",
       "i       209732\n",
       "a       208887\n",
       "to      208296\n",
       "it      177049\n",
       "of      143632\n",
       "is      130281\n",
       "this    108706\n",
       "in       95520\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(review_data_train['Summary_Clean_reviewtext']).split()).value_counts()[:10]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "one       34279\n",
       "like      29983\n",
       "good      25382\n",
       "show      24809\n",
       "great     23333\n",
       "well      21965\n",
       "would     20048\n",
       "use       20010\n",
       "get       19686\n",
       "really    17179\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_clean_column = pd.Series(' '.join(review_data_train['Clean']).split()).value_counts()[:10]\n",
    "freq_clean_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    wonderful car winter getting another color bra...\n",
       "1    xl fits perfect armored riding jacket lbs rush...\n",
       "2    since noticed wiper blades needed replacing pl...\n",
       "3    satisfied gv comfortable hold easy vibrate unc...\n",
       "4     amp job rating backup amp qsc gx sold using year\n",
       "Name: Clean, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_1 = list(freq_clean_column.index)\n",
    "review_data_train['Clean'] = review_data_train['Clean'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_1))\n",
    "review_data_train['Clean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    idea gardening lots hanging baskets container ...\n",
       "1    actually sleek looking although nerve actually...\n",
       "2    claustrophobic help imagine react stuck mudsli...\n",
       "3    beans purchased planted destroyed root opossum...\n",
       "4    amusing comedy mix politics court room drama t...\n",
       "Name: Clean, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_test['Clean'] = review_data_test['Clean'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_1))\n",
    "review_data_test['Clean'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rare words removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, just as we removed the most common words, this time let’s remove rarely occurring words from the text. Because they’re so rare, the association between them and other words is dominated by noise. You can replace rare words with a more general form and then this will have higher counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preseasoned       1\n",
       "instacloth        1\n",
       "kailyn            1\n",
       "reversable        1\n",
       "calbe             1\n",
       "cpk               1\n",
       "populous          1\n",
       "divulged          1\n",
       "lewdness          1\n",
       "bernsensummary    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(review_data_train['Summary_Clean_reviewtext']).split()).value_counts()[-10:]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "perferations    1\n",
       "placesdoes      1\n",
       "itneck          1\n",
       "refiting        1\n",
       "macgyverisms    1\n",
       "donegal         1\n",
       "tartaglia       1\n",
       "vandalia        1\n",
       "klan            1\n",
       "goddesses       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_clean_column = pd.Series(' '.join(review_data_train['Clean']).split()).value_counts()[-10:]\n",
    "freq_clean_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    wonderful car winter getting another color bra...\n",
       "1    xl fits perfect armored riding jacket lbs rush...\n",
       "2    since noticed wiper blades needed replacing pl...\n",
       "3    satisfied gv comfortable hold easy vibrate unc...\n",
       "4     amp job rating backup amp qsc gx sold using year\n",
       "Name: Clean, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_1 = list(freq_clean_column.index)\n",
    "review_data_train['Clean'] = review_data_train['Clean'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_1))\n",
    "review_data_train['Clean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    idea gardening lots hanging baskets container ...\n",
       "1    actually sleek looking although nerve actually...\n",
       "2    claustrophobic help imagine react stuck mudsli...\n",
       "3    beans purchased planted destroyed root opossum...\n",
       "4    amusing comedy mix politics court room drama t...\n",
       "Name: Clean, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_test['Clean'] = review_data_test['Clean'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_1))\n",
    "review_data_test['Clean'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these pre-processing steps are essential and help us in reducing our vocabulary clutter so that the features produced in the end are more effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve all seen tweets with a plethora of spelling mistakes. Our timelines are often filled with hastly sent tweets that are barely legible at times.\n",
    "\n",
    "In that regard, spelling correction is a useful pre-processing step because this also will help us in reducing multiple copies of words. For example, “Analytics” and “analytcs” will be treated as different words even if they are used in the same sense.\n",
    "\n",
    "To achieve this we will use the textblob library. If you are not familiar with it, you can check my previous article on ‘NLP for beginners using textblob’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    wonderful car winter getting another color bra...\n",
       "1    ll fits perfect armed riding jacket les rush p...\n",
       "2    since noticed wiped blades needed replacing pl...\n",
       "3    satisfied go comfortable hold easy vibrated un...\n",
       "4        amp job rating back amp is go sold using year\n",
       "Name: Clean, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "review_data_train['Clean'][:5].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    idea gardening lots hanging baskets container ...\n",
       "1    actually sleek looking although nerve actually...\n",
       "2    claustrophobic help imagine react stuck mudsli...\n",
       "3    beans purchased planted destroyed root opossum...\n",
       "4    amusing comedy mix politics court room drama t...\n",
       "Name: Clean, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_test['Clean'][:5].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization refers to dividing the text into a sequence of words or sentences. In our example, we have used the textblob library to first transform our tweets into a blob and then converted them into a series of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_cleaned_train = TextBlob(str(review_data_train['Clean'])).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['0', 'wonderful', 'car', 'winter', 'getting', 'another', 'color', 'bra', '1', 'xl', 'fits', 'perfect', 'armored', 'riding', 'jacket', 'lbs', 'rush', '2', 'since', 'noticed', 'wiper', 'blades', 'needed', 'replacing', 'pl', '3', 'satisfied', 'gv', 'comfortable', 'hold', 'easy', 'vibrate', 'unc', '4', 'amp', 'job', 'rating', 'backup', 'amp', 'qsc', 'gx', 'sold', 'using', 'year', '5', 'flushed', 'water', 'heater', 'garden', 'hose', 'could', 'feel', 'sm', '6', 'lots', 'mixed', 'reviews', 'thought', 'give', 'try', 'product', 'fi', '7', 'without', 'doubt', 'top', 'shows', 'television', 'right', 'top', 'n', '8', 'miata', 'actually', 'takes', 'hp', 'slightly', 'shorter', 'lengt', '9', 'many', 'think', 'especially', 'seen', 'crystal', 'meth', 'true', 'l', '10', 'works', 'baritone', 'ukulele', 'quality', 'full', 'size', 'acous', '11', 'heard', 'things', 'series', 'sampled', 'shows', 'last', 'night', 'a', '12', 'zombies', 'never', 'watched', 'made', 'see', 'new', 'way', 'way', 'dra', '13', 'less', 'expected', 'special', 'effects', 'wrapped', 'around', 'o', '14', 'bought', 'long', 'ago', 'got', 'installing', 'noticeably', 'loud', '15', 'love', 'betas', 'best', 'pilots', 'previewed', 'encourage', 'tur', '16', 'software', 'installed', 'fine', 'read', 'error', 'code', 'bmw', 'en', '17', 'enjoyed', 'watching', 'mark', 'taylor', 'flashpoint', 'sad', 'ch', '18', 'per', 'recommendation', 'cutting', 'stage', 'car', 'polishing', '19', 'series', 'loosely', 'based', 'craig', 'johnson', 'novels', 'stro', '20', 'perfect', 'pedal', 'easy', 'setup', 'easy', 'sounds', 'durable', 's', '21', 'guess', 'main', 'thing', 'hose', 'kink', 'said', 'less', 'hoses', 'ace', '22', 'installed', 'addition', 'python', 'system', 'sensor', 'simple', '23', 'small', 'mouse', 'problem', 'twice', 'year', 'mouse', 'finds', 'way', '24', 'months', 'completed', 'stopped', 'working', 'definitely', 'buy', '25', 'going', 'happen', 'next', 'season', 'spoiler', 'finding', 'plane', '26', 'best', 'filter', 'seen', 'far', 'ready', 'many', 'reviews', 'contru', '27', 'honestly', 'generally', 'care', 'romances', 'little', 'differ', '28', 'rest', 'episodes', 'preview', 'back', 'enjoy', 'listening', 'cla', '29', 'cable', 'came', 'time', 'promised', 'worked', 'immediately', 'up', '77045', 'deal', 'hitch', 'wiring', 'hardest', 'part', 'figuring', 'holes', '77046', 'vent', 'fit', 'perfect', 'replacing', 'old', 'breeze', 'ordered', '77047', 'looking', 'micro', 'fiber', 'towel', 'quick', 'detail', 'sprays', '77048', 'series', 'intended', 'present', 'personal', 'glimpse', 'depth', '77049', 'expecting', 'jack', 'end', 'jacks', 'actually', 'pretty', 'goes', '77050', 'put', 'car', 'emergencies', 'addition', 'emergency', 'kit', 'kno', '77051', 'year', 'old', 'granddaughter', 'purchased', 'supposed', 'watc', '77052', 'worth', 'time', 'drama', 'touch', 'action', 'suspense', 'series', '77053', 'enjoy', 'program', 'fun', 'watch', 'even', 'though', 'know', 'probl', '77054', 'amp', 'quite', 'time', 'find', 'exactly', 'want', 'much', 'easier', 'c', '77055', 'series', 'continues', 'composed', 'high', 'quality', 'writing', '77056', 'product', 'advertised', 'buy', 'makes', 'tires', 'nice', 'shiny', '77057', 'pretty', 'sad', 'movie', 'saw', 'mystery', 'science', 'guys', 'make', '77058', 'given', 'blade', 'free', 'test', 'pleased', 'wiper', 'blade', 'boug', '77059', 'got', 'meguiar', 'professional', 'dual', 'action', 'polisher', '77060', 'build', 'quality', 'look', 'gauge', 'pouch', 'nice', 'addition', 'c', '77061', 'dollars', 'replace', 'cabin', 'air', 'filter', 'carbon', 'activa', '77062', 'made', 'realize', 'best', 'programs', 'rarely', 'broadcast', 'tv', '77063', 'honda', 'accord', 'needs', 'wiper', 'blade', 'see', 'difference', '77064', 'can', 'not', 'say', 'enough', 'things', 'installed', 'keystone', 'pa', '77065', 'long', 'recall', 'never', 'heard', 'someone', 'say', 'live', 'anima', '77066', 'season', 'darker', 'character', 'development', 'episode', 'en', '77067', 'live', 'florida', 'drive', 'lot', 'also', 'plagued', 'bugs', 'hands', '77068', 'tried', 'several', 'various', 'mouse', 'control', 'methods', 'st', '77069', 'direct', 'replacement', 'part', 'b', 'grass', 'hog', 'model', 'nst', '77070', 'started', 'watching', 'got', 'hooked', 'characters', 'star', 'cr', '77071', 'put', 'new', 'battery', 'truck', 'usually', 'red', 'green', 'donuts', '77072', 'love', 'pedal', 'said', 'address', 'reviews', 'see', 'playing', 'gu', '77073', 'first', 'watched', 'pilot', 'saw', 'many', 'pretty', 'people', 'nic', '77074', 'gets', 'better', 'better', 'stories', 'wonderful', 'monroe', 'be', 'Name', 'Clean', 'Length', '77075', 'dtype', 'object'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " TextBlob(str(review_data_train['Clean'])).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_cleaned_test = TextBlob(str(review_data_test['Clean'][1])).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['actually', 'sleek', 'looking', 'although', 'nerve', 'actually', 'stick', 'guitar', 'lot', 'easier', 'find', 'guitar', 'bag', 'even', 'room', 'stray', 'picks', 'holds', 'medium', 'size', 'picks', 'sure', 'lose', 'lot', 'less', 'picks', 'almost', 'kind', 'fun', 'putting', 'pick', 'thing', 'weird', 'say', 'buy', 'already'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(review_data_test['Clean'][1]).words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming refers to the removal of suffices, like “ing”, “ly”, “s”, etc. by a simple rule-based approach. For this purpose, we will use PorterStemmer from the NLTK library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    wonder car winter get anoth color brand decid ...\n",
       "1    xl fit perfect armor ride jacket lb rush put c...\n",
       "2    sinc notic wiper blade need replac pleas see i...\n",
       "3    satisfi gv comfort hold easi vibrat uncomfort ...\n",
       "4         amp job rate backup amp qsc gx sold use year\n",
       "Name: Clean, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "review_data_train['Clean'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    idea garden lot hang basket contain plant pati...\n",
       "1    actual sleek look although nerv actual stick g...\n",
       "2    claustrophob help imagin react stuck mudslid t...\n",
       "3    bean purchas plant destroy root opossum numer ...\n",
       "4    amus comedi mix polit court room drama ton fun...\n",
       "Name: Clean, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data_test['Clean'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is a more effective option than stemming because it converts the word into its root word, rather than just stripping the suffices. It makes use of the vocabulary and does a morphological analysis to obtain the root word. Therefore, we usually prefer using lemmatization over stemming.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-81ab1b411ab9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreview_data_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Clean_lemmed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview_data_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Clean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mreview_data_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Clean_lemmed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-81ab1b411ab9>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreview_data_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Clean_lemmed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview_data_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Clean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mreview_data_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Clean_lemmed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-81ab1b411ab9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreview_data_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Clean_lemmed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview_data_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Clean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mreview_data_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Clean_lemmed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/textblob/decorators.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/textblob/blob.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, pos)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_penn_to_wordnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mPorterStemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_morphy\u001b[0;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;31m# 2. Return all that are in the database (and check the original too)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_forms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36mfilter_forms\u001b[0;34m(forms)\u001b[0m\n\u001b[1;32m   1829\u001b[0m             \u001b[0mseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1831\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lemma_pos_offset_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1832\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lemma_pos_offset_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1833\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mform\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from textblob import Word\n",
    "review_data_train['Clean_lemmed'] = review_data_train['Clean'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "review_data_train['Clean_lemmed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data_test['Clean_lemmed'] = review_data_test['Clean'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "review_data_test['Clean_lemmed'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advance Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, we have done all the basic pre-processing steps in order to clean our data. Now, we can finally move on to extracting features using NLP techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams are the combination of multiple words used together. Ngrams with N=1 are called unigrams. Similarly, bigrams (N=2), trigrams (N=3) and so on can also be used.\n",
    "\n",
    "Unigrams do not usually contain as much information as compared to bigrams and trigrams. The basic principle behind n-grams is that they capture the language structure, like what letter or word is likely to follow the given one. The longer the n-gram (the higher the n), the more context you have to work with. Optimum length really depends on the application – if your n-grams are too short, you may fail to capture important differences. On the other hand, if they are too long, you may fail to capture the “general knowledge” and only stick to particular cases.\n",
    "\n",
    "So, let’s quickly extract bigrams from our tweets using the ngrams function of the textblob library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextBlob(review_data_train['Clean_lemmed'][0]).ngrams(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextBlob(review_data_test['Clean_lemmed'][0]).ngrams(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequency is simply the ratio of the count of a word present in a sentence, to the length of the sentence.\n",
    "\n",
    "Therefore, we can generalize term frequency as:\n",
    "\n",
    "TF = (Number of times term T appears in the particular row) / (number of terms in that row)\n",
    "\n",
    "Below, I have tried to show you the term frequency table of a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1 = (review_data_train['Clean_lemmed'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "tf1.columns = ['words','tf']\n",
    "tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1_test = (review_data_test['Clean_lemmed'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "tf1_test.columns = ['words','tf']\n",
    "tf1_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind inverse document frequency (IDF) is that a word is not of much use to us if it’s appearing in all the documents.\n",
    "\n",
    "Therefore, the IDF of each word is the log of the ratio of the total number of rows to the number of rows in which that word is present.\n",
    "\n",
    "IDF = log(N/n), where, N is the total number of rows and n is the number of rows in which the word was present.\n",
    "\n",
    "So, let’s calculate IDF for the same tweets for which we calculated the term frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,word in enumerate(tf1['words']):\n",
    "  tf1.loc[i, 'idf'] = np.log(review_data_train.shape[0]/(len(review_data_train[review_data_train['Clean_lemmed'].str.contains(word)])))\n",
    "\n",
    "tf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,word in enumerate(tf1_test['words']):\n",
    "  tf1_test.loc[i, 'idf'] = np.log(review_data_test.shape[0]/(len(review_data_test[review_data_test['Clean_lemmed'].str.contains(word)])))\n",
    "\n",
    "tf1_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more the value of IDF, the more unique is the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency – Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is the multiplication of the TF and IDF which we calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1['tfidf'] = tf1['tf'] * tf1['idf']\n",
    "tf1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1_test['tfidf'] = tf1_test['tf'] * tf1_test['idf']\n",
    "tf1_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the TF-IDF has penalized words like ‘don’t’, ‘can’t’, and ‘use’ because they are commonly occurring words. However, it has given a high weight to “disappointed” since that will be very useful in determining the sentiment of the tweet.\n",
    "\n",
    "We don’t have to calculate TF and IDF every time beforehand and then multiply it to obtain TF-IDF. Instead, sklearn has a separate function to directly obtain it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n",
    " stop_words= 'english',ngram_range=(1,1))\n",
    "train_vect = tfidf.fit_transform(review_data_train['Clean_lemmed'])\n",
    "\n",
    "train_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vect = tfidf.fit_transform(review_data_test['Clean_lemmed'])\n",
    "test_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words (BoW) refers to the representation of text which describes the presence of words within the text data. The intuition behind this is that two similar text fields will contain similar kind of words, and will therefore have a similar bag of words. Further, that from the text alone we can learn something about the meaning of the document.\n",
    "\n",
    "For implementation, sklearn provides a separate function for it as shown below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n",
    "train_bow = bow.fit_transform(review_data_train['Clean_lemmed'])\n",
    "train_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bow = bow.fit_transform(review_data_test['Clean_lemmed'])\n",
    "test_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you recall, our problem was to detect the ratings of the reviewtexts. So, before applying any ML/DL models (which can have a separate feature detecting the sentiment using the textblob library), let’s check the ratings of the first few reviewtexts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data_train['Clean_lemmed'][:5].apply(lambda x: TextBlob(x).sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data_test['Clean_lemmed'][:5].apply(lambda x: TextBlob(x).sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you can see that it returns a tuple representing polarity and subjectivity of each tweet. Here, we only extract polarity as it indicates the sentiment as value nearer to 1 means a positive sentiment and values nearer to -1 means a negative sentiment. This can also work as a feature for building a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data_train['sentiment_rating'] = review_data_train['Clean_lemmed'].apply(lambda x: TextBlob(x).sentiment[0] )\n",
    "review_data_train[['Clean_lemmed','sentiment_rating']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data_test['sentiment_rating'] = review_data_test['Clean_lemmed'].apply(lambda x: TextBlob(x).sentiment[0] )\n",
    "review_data_test[['Clean_lemmed','sentiment_rating']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embedding is the representation of text in the form of vectors. The underlying idea here is that similar words will have a minimum distance between their vectors.\n",
    "\n",
    "Word2Vec models require a lot of text, so either we can train it on our training data or we can use the pre-trained word vectors developed by Google, Wiki, etc.\n",
    "\n",
    "Here, we will use pre-trained word vectors which can be downloaded from the glove website. There are different dimensions (50,100, 200, 300) vectors trained on wiki data. For this example, I have downloaded the 100-dimensional version of the model.\n",
    "\n",
    "The first step here is to convert it into the word2vec format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = './glove/glove.6B.300d.txt'\n",
    "word2vec_output_file = './glove/glove.6B.300d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the above word2vec file as a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors # load the Stanford GloVe model\n",
    "filename = './glove/glove.6B.300d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s say our review text contains a text saying ‘go away’. We can easily obtain it’s word vector using the above model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['go']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['away']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then take the average to represent the string ‘go away’ in the form of vectors having 100 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(model['go'] + model['away'])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have converted the entire string into a vector which can now be used as a feature in any modelling technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgdhjasgdkjshg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model[model.wv.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.scatter(result[:, 0], result[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the screenshots for the above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"./Image/Vocab.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"./Image/Annotation.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing wordcloud to anyalize the words present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "mpl.rcParams['font.size']=12                #10 \n",
    "mpl.rcParams['savefig.dpi']=100             #72 \n",
    "mpl.rcParams['figure.subplot.bottom']=.1 \n",
    "\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='black',\n",
    "        max_words=300,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "        \n",
    "    ).generate(str(data))\n",
    "    \n",
    "    fig = plt.figure(1, figsize=(15, 15))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "    \n",
    "show_wordcloud(review_data_train[\"Clean_lemmed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "mpl.rcParams['font.size']=12                #10 \n",
    "mpl.rcParams['savefig.dpi']=100             #72 \n",
    "mpl.rcParams['figure.subplot.bottom']=.1 \n",
    "\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='black',\n",
    "        max_words=300,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "        \n",
    "    ).generate(str(data))\n",
    "    \n",
    "    fig = plt.figure(1, figsize=(15, 15))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "    \n",
    "show_wordcloud(review_data_test[\"Clean_lemmed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "mpl.rcParams['font.size']=12                #10 \n",
    "mpl.rcParams['savefig.dpi']=100             #72 \n",
    "mpl.rcParams['figure.subplot.bottom']=.1 \n",
    "\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='black',\n",
    "        max_words=300,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "        \n",
    "    ).generate(str(data))\n",
    "    \n",
    "    fig = plt.figure(1, figsize=(15, 15))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "    \n",
    "show_wordcloud(review_data_train[\"reviewText\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "mpl.rcParams['font.size']=12                #10 \n",
    "mpl.rcParams['savefig.dpi']=100             #72 \n",
    "mpl.rcParams['figure.subplot.bottom']=.1 \n",
    "\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='black',\n",
    "        max_words=300,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "        \n",
    "    ).generate(str(data))\n",
    "    \n",
    "    fig = plt.figure(1, figsize=(15, 15))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "    \n",
    "show_wordcloud(review_data_test[\"reviewText\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "mpl.rcParams['font.size']=12                #10 \n",
    "mpl.rcParams['savefig.dpi']=100             #72 \n",
    "mpl.rcParams['figure.subplot.bottom']=.1 \n",
    "\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='black',\n",
    "        max_words=300,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "        \n",
    "    ).generate(str(data))\n",
    "    \n",
    "    fig = plt.figure(1, figsize=(15, 15))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "    \n",
    "show_wordcloud(review_data_train[[\"reviewText\", \"overall\"]][review_data_train.overall==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "mpl.rcParams['font.size']=12                #10 \n",
    "mpl.rcParams['savefig.dpi']=100             #72 \n",
    "mpl.rcParams['figure.subplot.bottom']=.1 \n",
    "\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='black',\n",
    "        max_words=300,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "        \n",
    "    ).generate(str(data))\n",
    "    \n",
    "    fig = plt.figure(1, figsize=(15, 15))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "    \n",
    "show_wordcloud(review_data_train[[\"reviewText\", \"overall\"]][review_data_train.overall==2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "mpl.rcParams['font.size']=12                #10 \n",
    "mpl.rcParams['savefig.dpi']=100             #72 \n",
    "mpl.rcParams['figure.subplot.bottom']=.1 \n",
    "\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='black',\n",
    "        max_words=300,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "        \n",
    "    ).generate(str(data))\n",
    "    \n",
    "    fig = plt.figure(1, figsize=(15, 15))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "    \n",
    "show_wordcloud(review_data_train[[\"reviewText\", \"overall\"]][review_data_train.overall==3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "mpl.rcParams['font.size']=12                #10 \n",
    "mpl.rcParams['savefig.dpi']=100             #72 \n",
    "mpl.rcParams['figure.subplot.bottom']=.1 \n",
    "\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='black',\n",
    "        max_words=300,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "        \n",
    "    ).generate(str(data))\n",
    "    \n",
    "    fig = plt.figure(1, figsize=(15, 15))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "    \n",
    "show_wordcloud(review_data_train[[\"reviewText\", \"overall\"]][review_data_train.overall==4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "mpl.rcParams['font.size']=12                #10 \n",
    "mpl.rcParams['savefig.dpi']=100             #72 \n",
    "mpl.rcParams['figure.subplot.bottom']=.1 \n",
    "\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='black',\n",
    "        max_words=300,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "        \n",
    "    ).generate(str(data))\n",
    "    \n",
    "    fig = plt.figure(1, figsize=(15, 15))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "    \n",
    "show_wordcloud(review_data_train[[\"reviewText\", \"overall\"]][review_data_train.overall==5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method with bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =review_data_train[\"Clean_lemmed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=1, max_features=1000, stop_words='english')\n",
    "# bag-of-words feature matrix\n",
    "bow = bow_vectorizer.fit_transform(x['Clean_lemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_train = bow.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow_X, val_bow_X, train_bow_y, val_bow_y = train_test_split(bow_train, review_data_train[\"overall\"], test_size = 0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow_X = pd.DataFrame(train_bow_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_bow_X = pd.DataFrame(val_bow_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow_y = pd.DataFrame(train_bow_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_bow_y = pd.DataFrame(val_bow_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg=LogisticRegression()\n",
    "logreg.fit(train_bow_X,train_bow_y)\n",
    "y_pred=logreg.predict(val_bow_X)\n",
    "\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set:',logreg.score(val_bow_X, val_bow_y))\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(val_bow_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, accuracy_score\n",
    "\n",
    "acc = accuracy_score(val_bow_y, y_pred)\n",
    "\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## k fold cross validation accuracy on train data is 63%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "kfold = model_selection.KFold(n_splits=15, random_state=7)\n",
    "modelCV = LogisticRegression()\n",
    "scoring = 'accuracy'\n",
    "results = model_selection.cross_val_score(modelCV, train_tfidf_X, train_tfidf_y, cv=kfold, scoring=scoring)\n",
    "print(\"10-fold cross validation average accuracy: \", (results.mean()))\n",
    "modelCV.fit(train_tfidf_X, train_tfidf_y)\n",
    "preds1 = modelCV.predict(val_tfidf_X)\n",
    "confusion_matrix(val_tfidf_y.astype(int),preds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(val_tfidf_y, preds1)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = modelCV.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = pd.DataFrame(preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"ID\": review_data_test[\"ID\"],\n",
    "        \"overall\": preds_test[0]\n",
    "    })\n",
    "\n",
    "submission.to_csv('./Submission/submission_tfiddf_log_reg_cross_validation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =review_data_test[\"Clean_lemmed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "# bag-of-words feature matrix\n",
    "bow_test1 = bow_vectorizer.fit_transform(y['Clean_lemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_test = bow_test1.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_test = pd.DataFrame(bow_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_log_reg =logreg.predict(bow_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_log_reg = pd.DataFrame(y_pred_test_log_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_log_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"ID\": review_data_test[\"ID\"],\n",
    "        \"overall\": y_pred_test_log_reg[0]\n",
    "    })\n",
    "\n",
    "submission.to_csv('./Submission/submission_log_reg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tfidf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=9000, stop_words='english')\n",
    "# TF-IDF feature matrix\n",
    "tfidf = tfidf_vectorizer.fit_transform(x['Clean_lemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train = tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf_X, val_tfidf_X, train_tfidf_y, val_tfidf_y = train_test_split(tfidf_train, review_data_train[\"overall\"], test_size = 0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf_X = pd.DataFrame(train_tfidf_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf_y = pd.DataFrame(train_tfidf_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tfidf_X = pd.DataFrame(val_tfidf_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tfidf_y = pd.DataFrame(val_tfidf_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg=LogisticRegression()\n",
    "logreg.fit(train_tfidf_X,train_tfidf_y)\n",
    "y_pred_tfidf=logreg.predict(val_tfidf_X)\n",
    "\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set:',logreg.score(val_tfidf_X, val_tfidf_y))\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(val_tfidf_y, y_pred_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, accuracy_score\n",
    "\n",
    "acc = accuracy_score(val_tfidf_y, y_pred_tfidf)\n",
    "\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb=GaussianNB()\n",
    "gnb.fit(train_tfidf_X,train_tfidf_y)\n",
    "y_pred_tfidf=gnb.predict(val_tfidf_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of lgaussian naive bayes classifier on test set:',gnb.score(val_tfidf_X, val_tfidf_y))\n",
    "\n",
    "#18%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc = accuracy_score(val_tfidf_y, y_pred_tfidf)\n",
    "\n",
    "\n",
    "print(acc)\n",
    "#18%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb=MultinomialNB()\n",
    "mnb.fit(train_tfidf_X,train_tfidf_y)\n",
    "y_pred_tfidf_mnb=mnb.predict(val_tfidf_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of logistic regression classifier on test set:',mnb.score(val_tfidf_X, val_tfidf_y))\n",
    "acc = accuracy_score(val_tfidf_y, y_pred_tfidf_mnb)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test part with tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=9000, stop_words='english')\n",
    "# TF-IDF feature matrix\n",
    "tfidf_test = tfidf_vectorizer.fit_transform(y['Clean_lemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = tfidf_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = pd.DataFrame(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tfidf_test_log_reg =logreg.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tfidf_test_log_reg = pd.DataFrame(y_pred_tfidf_test_log_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tfidf_test_log_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"ID\": review_data_test[\"ID\"],\n",
    "        \"overall\": y_pred_tfidf_test_log_reg[0]\n",
    "    })\n",
    "\n",
    "submission.to_csv('./Submission/submission_tfiddf_log_reg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb=XGBClassifier()\n",
    "xgb.fit(train_tfidf_X,train_tfidf_y)\n",
    "y_pred_tfidf=xgb.predict(val_tfidf_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of logistic regression classifier on test set:',mnb.score(val_tfidf_X, val_tfidf_y))\n",
    "acc = accuracy_score(val_tfidf_y, y_pred_tfidf_mnb)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With few models together models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Classifier Modules\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(train_tfidf_X, train_tfidf_y)\n",
    "y_pred_log_reg = clf.predict(val_tfidf_X)\n",
    "acc_log_reg_train = round( clf.score(train_tfidf_X, train_tfidf_y) * 100, 2)\n",
    "print ('train_accuracy_log_reg' +' '+str(acc_log_reg_train) + ' percent')\n",
    "acc_log_reg_test = accuracy_score(val_tfidf_y, y_pred_log_reg)\n",
    "print('test_accuracy_log_reg '+str(round((acc_log_reg_test)*100,2)) + 'percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUpport vector classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC()\n",
    "clf.fit(train_tfidf_X, train_tfidf_y)\n",
    "y_pred_svc = clf.predict(val_tfidf_X)\n",
    "acc_svc_train = round(clf.score(train_tfidf_X, train_tfidf_y) * 100, 2)\n",
    "print ('train_accuracy_svc '+str(acc_svc_train) + ' percent')\n",
    "acc_svc_test = accuracy_score(val_tfidf_y, y_pred_svc)\n",
    "print('test_accuracy_svc '+str(round((acc_svc_test)*100,2)) + 'percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(train_tfidf_X, train_tfidf_y)\n",
    "y_pred_linear_svm = clf.predict(val_tfidf_X)\n",
    "acc_linear_svm_train = round(clf.score(train_tfidf_X, train_tfidf_y) * 100, 2)\n",
    "print ('train_accuracy_svm '+str(acc_linear_svm_train) + ' percent')\n",
    "acc_svm_test = accuracy_score(val_tfidf_y, y_pred_linear_svm)\n",
    "print('test_accuracy_svm '+str(round((acc_svm_test)*100,2)) + 'percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors = 3)\n",
    "clf.fit(train_tfidf_X, train_tfidf_y)\n",
    "y_pred_knn = clf.predict(val_tfidf_X)\n",
    "acc_knn_train = round(clf.score(train_tfidf_X, train_tfidf_y) * 100, 2)\n",
    "print ('train_accuracy_knn '+str(acc_knn_train) + ' percent')\n",
    "acc_knn_test = accuracy_score(val_tfidf_y, y_pred_knn)\n",
    "print('test_accuracy_knn '+str(round((acc_knn_test)*100,2)) + 'percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(train_tfidf_X, train_tfidf_y)\n",
    "y_pred_decision_tree = clf.predict(val_tfidf_X)\n",
    "acc_decision_tree_train = round(clf.score(train_tfidf_X, train_tfidf_y) * 100, 2)\n",
    "print ('train_accuracy_decision tree '+str(acc_decision_tree_train) + ' percent')\n",
    "acc_decision_tree_test = accuracy_score(val_tfidf_y,y_pred_decision_tree)\n",
    "print('test_accuracy_decision tree '+str(round((acc_decision_tree_test)*100,2)) + 'percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(train_tfidf_X, train_tfidf_y)\n",
    "y_pred_random_forest = clf.predict(val_tfidf_X)\n",
    "acc_random_forest_train = round(clf.score(train_tfidf_X, train_tfidf_y) * 100, 2)\n",
    "print ('train_accuracy_random_forest '+str(acc_random_forest_train) + ' percent')\n",
    "acc_random_forest_test = accuracy_score(val_tfidf_y,y_pred_random_forest)\n",
    "print('test_accuracy_random_forest '+str(round((acc_random_forest_test)*100,2)) + 'percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(train_tfidf_X, train_tfidf_y)\n",
    "y_pred_gnb = clf.predict(val_tfidf_X)\n",
    "acc_gnb_train = round(clf.score(train_tfidf_X, train_tfidf_y) * 100, 2)\n",
    "print ('train_accuracy_gnb '+str(acc_gnb_train) + ' percent')\n",
    "acc_gnb_test = accuracy_score(val_tfidf_y,y_pred_gnb)\n",
    "print('test_accuracy_gnb '+str(round((acc_gnb_test)*100,2)) + 'percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Perceptron(max_iter=5, tol=None)\n",
    "clf.fit(train_tfidf_X, train_tfidf_y)\n",
    "y_pred_perceptron = clf.predict(val_tfidf_X)\n",
    "acc_perceptron_train = round(clf.score(train_tfidf_X, train_tfidf_y) * 100, 2)\n",
    "print ('train_accuracy_perceptron '+str(acc_perceptron_train) + ' percent')\n",
    "acc_perceptron_test = accuracy_score(val_tfidf_y,y_pred_perceptron)\n",
    "print('test_accuracy_perceptron '+str(round((acc_perceptron_test)*100,2)) + 'percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(max_iter=5, tol=None)\n",
    "clf.fit(train_tfidf_X, train_tfidf_y)\n",
    "y_pred_sgd = clf.predict(val_tfidf_X)\n",
    "acc_sgd_train = round(clf.score(train_tfidf_X, train_tfidf_y) * 100, 2)\n",
    "print ('train_accuracy_sgd '+str(acc_sgd_train) + ' percent')\n",
    "acc_sgd_test = accuracy_score(val_tfidf_y,y_pred_sgd)\n",
    "print('test_accuracy_sgd '+str(round((acc_sgd_test)*100,2)) + 'percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', \n",
    "                'Decision Tree', 'Naive Bayes', \n",
    "              'Perceptron', 'Stochastic Gradient Decent'],\n",
    "    \n",
    "    'Score_train': [acc_log_reg_train, \n",
    "             acc_decision_tree_train,  \n",
    "                acc_gnb_train, acc_perceptron_train, acc_sgd_train],\n",
    "    \n",
    "    'Score_test': [acc_log_reg_test,\n",
    "                  acc_decision_tree_test,\n",
    "                  acc_gnb_test, acc_perceptron_test,\n",
    "                  acc_sgd_test]\n",
    "    })\n",
    "\n",
    "models.sort_values(by='Score_train', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.sort_values(by='Score_test', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"./Image/models.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedddings_ binary_cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_np_utils = np_utils.to_categorical(review_data_train[\"overall\"]-1,num_classes =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaned_train = review_data_train['Clean_lemmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaned_test = review_data_test['Clean_lemmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = list(review_data_train[\"overall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "from numpy import asarray\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_cleaned_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(text_cleaned_train)\n",
    "\n",
    "print (t.word_index)\n",
    "\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print (vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(text_cleaned_train)\n",
    "#print(text_cleaned_train)\n",
    "print(encoded_docs[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(encoded_docs)[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length=[]\n",
    "for i in range(0,77074):\n",
    "    length.append(len(encoded_docs[i]))\n",
    "max(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = 1605\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(encoded_docs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs_norm = normalize(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=9000))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_np_utils = labels_np_utils[:53952,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_np_utils.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(train_tfidf_X, labels_np_utils, validation_split= 0.3, epochs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = pd.DataFrame(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tfidf_test_log_reg =model.predict_classes(tfidf_test)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tfidf_test_log_reg = pd.DataFrame(y_pred_tfidf_test_log_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tfidf_test_log_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"ID\": review_data_test[\"ID\"],\n",
    "        \"overall\": y_pred_tfidf_test_log_reg[0]\n",
    "    })\n",
    "\n",
    "submission.to_csv('./Submission/submission_tfiddf_ANN_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(train_tfidf_X, labels_np_utils, validation_split= 0.3, epochs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf_test = tfidf_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = pd.DataFrame(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tfidf_test_log_reg =model.predict_classes(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tfidf_test_log_reg = pd.DataFrame(y_pred_tfidf_test_log_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tfidf_test_log_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"ID\": review_data_test[\"ID\"],\n",
    "        \"overall\": y_pred_tfidf_test_log_reg[0]\n",
    "    })\n",
    "\n",
    "submission.to_csv('./Submission/submission_tfiddf_ANN_categrorical_crossentropy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model lstm\n",
    "embedding_vecor_length = 64\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, 1, input_length=9000))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(10))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(train_tfidf_X, labels_np_utils, validation_split= 0.3, epochs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = tfidf_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = pd.DataFrame(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tfidf_test_log_reg =model.predict_classes(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tfidf_test_log_reg = pd.DataFrame(y_pred_tfidf_test_log_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tfidf_test_log_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"ID\": review_data_test[\"ID\"],\n",
    "        \"overall\": y_pred_tfidf_test_log_reg[0]\n",
    "    })\n",
    "\n",
    "submission.to_csv('./Submission/submission_tfiddf_ANN.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vecor_length, input_length=9000))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(train_tfidf_X, labels_np_utils, validation_split= 0.3, epochs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = tfidf_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = pd.DataFrame(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tfidf_test_log_reg =model.predict_classes(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tfidf_test_log_reg = pd.DataFrame(y_pred_tfidf_test_log_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tfidf_test_log_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"ID\": review_data_test[\"ID\"],\n",
    "        \"overall\": y_pred_tfidf_test_log_reg[0]\n",
    "    })\n",
    "\n",
    "submission.to_csv('./Submission/submission_tfiddf_ANN.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN did not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the input that was there was image in the pretrained models.\n",
    "So went ahead with building using glove and word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method of word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_np_utils = np_utils.to_categorical(review_data_train[\"overall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaned_train = review_data_train['Clean_lemmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaned_test = review_data_test['Clean_lemmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = list(review_data_train[\"overall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "from numpy import asarray\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_cleaned_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(text_cleaned_train)\n",
    "\n",
    "print (t.word_index)\n",
    "\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print (vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(text_cleaned_train)\n",
    "#print(text_cleaned_train)\n",
    "print(encoded_docs[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(encoded_docs)[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length=[]\n",
    "for i in range(0,77074):\n",
    "    length.append(len(encoded_docs[i]))\n",
    "max(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = 1605\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(encoded_docs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs_norm = normalize(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(train_tfidf_X, labels_np_utils, validation_split= 0.3, epochs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"./Image/DL_1.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('./glove/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl_X_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cool =embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cool_1=cool[:53952]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cool_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Embedding(53952, 100, weights=[cool_1], input_length=9000, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    cool_1[i] = embedding_vector# fit the model\n",
    "model.fit(train_tfidf_X, labels_np_utils, validation_split=0.3, epochs=3, verbose=1)    cool_1[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "#model.fit(train_dl_X_list, label_train_y, validation_split= 0.3, epochs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using glove 300D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('./glove/glove.6B.300d.txt.word2vec')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 300))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl_X_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cool =embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cool_1=cool[:53952]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cool_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Embedding(53952, 300, weights=[cool_1], input_length=9000, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(train_tfidf_X, labels_np_utils, validation_split= 0.3, epochs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"./Image/Dl_300_glove.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"./Image/Dl_301_glove.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"./Image/Dl_302_glove.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"./Image/Dl_303_glove.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with pretrained embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, LSTM, Dropout, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words=9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model lstm\n",
    "embedding_vecor_length = 32\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(e)\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(train_tfidf_X, labels_np_utils, validation_split= 0.3, epochs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"./Image/lstm_e_1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"./Image/lstm_e_2.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"./Image/lstm_e_3.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM without pretrained embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words=9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model lstm\n",
    "embedding_vecor_length = 32\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=9000))\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(train_tfidf_X, labels_np_utils, validation_split= 0.3, epochs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method  logistic regression is giving 63% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =review_data_train[\"Clean_lemmed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.column= \"reviewtext\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sklearn_tfidf = TfidfVectorizer(norm='l2',min_df=0, smooth_idf=True, tokenizer=None)\n",
    "\n",
    "sklearn_representation = sklearn_tfidf.fit_transform(x.Clean_lemmed)\n",
    "print(sklearn_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy arrays are easy to work with, so convert the result to an array\n",
    "data_features = sklearn_representation.toarray()\n",
    "print(data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features=pd.DataFrame(data_features)\n",
    "print(data_features)\n",
    "print(data_features.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features[\"overall\"]=review_data_train[\"overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(data_features, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [col for col in data_features.columns if col not in [\"overall\"]]\n",
    "\n",
    "train.x= train[cols]\n",
    "train.y=train[\"overall\"]\n",
    "\n",
    "test.x=test[cols]\n",
    "test.y=test[\"overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg=LogisticRegression()\n",
    "logreg.fit(train.x,train.y)\n",
    "y_pred=logreg.predict(test.x)\n",
    "\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set:',logreg.score(test.x, test.y))\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test.y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, accuracy_score\n",
    "\n",
    "acc = accuracy_score(test.y, y_pred)\n",
    "\n",
    "\n",
    "print(acc)\n",
    "\n",
    "# 63% accuracy on validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Cross Validation for Logistic Regression Model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "kfold = model_selection.KFold(n_splits=15, random_state=7)\n",
    "modelCV = LogisticRegression()\n",
    "scoring = 'accuracy'\n",
    "results = model_selection.cross_val_score(modelCV, train.x, train.y, cv=kfold, scoring=scoring)\n",
    "print(\"10-fold cross validation average accuracy: \", (results.mean()))\n",
    "modelCV.fit(train.x, train.y)\n",
    "preds1 = modelCV.predict(test.x)\n",
    "confusion_matrix(test.y.astype(int),preds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Building Naive Bayes Model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(train.x, train.y).predict(test.x)\n",
    "confusion_matrix(test.y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_nb = metrics.accuracy_score(test.y, y_pred)\n",
    "print(accuracy_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os \n",
    "import re  \n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from numpy import zeros\n",
    "from numpy import asarray\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 543\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"train.csv\")\n",
    "test=pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"ID\",axis=1)\n",
    "test = test.drop(\"ID\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.dropna(axis = 0, how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(axis = 0, how = 'any' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.groupby(\"overall\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re.sub\n",
    "test.head()\n",
    "#data['reviewText'][436]\n",
    "#re.sub(r'[0-9_]+', '', data['reviewText'][1])\n",
    "#print(data['reviewText'][436])\n",
    "#print(text_cleaner(data['reviewText'][436]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "t = WordPunctTokenizer()\n",
    "\n",
    "pattern = r'[0-9_]+'\n",
    "\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\", \"aint\":\"are not\", \"wont\":\"will not\", \"aren\":\"are not\", \"dont\":\"do not\",\n",
    "                \"cant\":\"can not\", \"dunno\":\"do not know\", 'idk':\"i do not know\"}\n",
    "\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\n",
    "             \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'us'\n",
    "             'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', \n",
    "             'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', \n",
    "             'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', \n",
    "             'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', \n",
    "             'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', \n",
    "             'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', \n",
    "             'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', \n",
    "             'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', \n",
    "             'how', 'all', 'any', 'both', 'each','other', 'some', 'such', 'own', 'same', 'so', 'than', \n",
    "             's', 't',  'would','can', 'will', 'just', 'should', \"should've\", 'now', 'd', 'll', 'm', \n",
    "             'o', 're', 've', 'y']\n",
    "\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def text_cleaner(text):\n",
    "    \n",
    "    text = re.sub(pattern, '', text)\n",
    "#    text= text.lower()\n",
    "    text = neg_pattern.sub(lambda x: negations_dic[x.group()], text)\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    #To remove double or more whitespaces\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    \n",
    "    #Tokenize the text\n",
    "    tokens = [x for x  in t.tokenize(text) if len(x) > 1]\n",
    "    \n",
    "    #Remove Stop words\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    \n",
    "    tokens = [lmtzr.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return (\" \".join(tokens)).strip()\n",
    "    #return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Data Clean\n",
    "print(\"Cleaning the Train data...\\n\")\n",
    "clean_texts = []\n",
    "for i in range(0,len(data)):                                                                \n",
    "    clean_texts.append(text_cleaner(data['reviewText'][i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Data Clean\n",
    "print(\"Cleaning the Test data...\\n\")\n",
    "clean_test = []\n",
    "for i in range(0,len(test)):                                                                \n",
    "    clean_test.append(text_cleaner(test['reviewText'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.DataFrame(clean_texts,columns=['reviewText'])\n",
    "clean_df['overall'] = data.overall\n",
    "clean_df.to_csv('clean_text.csv',encoding='utf-8')\n",
    "\n",
    "test_df = pd.DataFrame(clean_test,columns=['reviewText'])\n",
    "test_df.to_csv('clean_test.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCloud_String(text,rating):\n",
    "    text_cat1 = clean_df[clean_df.overall == rating]\n",
    "    cat1_string = []\n",
    "    for t in text_cat1.reviewText:\n",
    "        cat1_string.append(t)\n",
    "    return(pd.Series(cat1_string).str.cat(sep=' '))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud for Rating 1\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "for i in range(1,6):\n",
    "    print(\"Word Cloud for Rating %d\" % i)\n",
    "    wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(wordCloud_String(clean_df.overall,i))\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(clean_df[\"reviewText\"])\n",
    "\n",
    "print (t.word_index)\n",
    "\n",
    "vocab_size = len(t.word_index) + 1 \n",
    "print (vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting data into train & test\n",
    "train_docs= clean_df[\"reviewText\"]\n",
    "train_labels=clean_df[\"overall\"]\n",
    "\n",
    "\n",
    "test_docs = test_df[\"reviewText\"]\n",
    "# docs = clean_df[\"reviewText\"]\n",
    "# labels = clean_df[\"overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode the documents\n",
    "#encoded_docs = t.texts_to_sequences(clean_df[\"reviewText\"])\n",
    "train_encoded_docs = t.texts_to_sequences(train_docs)\n",
    "test_encoded_docs = t.texts_to_sequences(test_docs)\n",
    "#clean_df[\"reviewText\"] = t.texts_to_sequences(clean_df[\"reviewText\"])\n",
    "# print(docs)\n",
    "#clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def max_length(reviews):\n",
    "#     max = 0 \n",
    "#     for doc in reviews:\n",
    "#         if len(doc)>max: max = len(doc)\n",
    "#     return max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_review_length = 500\n",
    "# #print(max_length(docs))\n",
    "# num = []\n",
    "# for doc in docs:\n",
    "#     num.append(len(doc))\n",
    "# num.sort(reverse=True)\n",
    "# print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad documents to a max length of 4 words\n",
    "\n",
    "train_padded_docs = pad_sequences(train_encoded_docs, maxlen=max_review_length, padding='post')\n",
    "test_padded_docs = pad_sequences(test_encoded_docs, maxlen=max_review_length, padding='post')\n",
    "# labels = clean_df[\"overall\"].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = (train_labels.values-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_padded_docs)\n",
    "type(train_labels)\n",
    "#print(len(numpy.unique(numpy.hstack(train_labels))))\n",
    "#numpy.concatenate((train_padded_docs, train_labels), axis=0)\n",
    "\n",
    "\n",
    "from keras.utils.np_utils import to_categorical   \n",
    "\n",
    "train_labels_c = to_categorical(train_labels)\n",
    "#print(len(numpy.unique(numpy.hstack(train_labels))))#valid_labels = to_categorical(valid_labels, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(numpy.unique(numpy.hstack(train_labels))))\n",
    "print(train_labels[1])\n",
    "print(train_labels_c[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt',encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=500, trainable=False,))\n",
    "    model.add(Dense(32,activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    ##Compile Model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'],)\n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, epochs = 50, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = cross_val_score(estimator,train_padded_docs,train_labels_c,cv=kfold )\n",
    "#print(\"Baseline: %.2f%% (%.2f%%)\") % (results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, Y_train, Y_test = train_test_split(train_padded_docs, train_labels, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(train_padded_docs, train_labels)\n",
    "predictions = estimator.predict(test_padded_docs)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"test.csv\")\n",
    "df1.dropna(how='any',axis=0, inplace=True)\n",
    "df1.drop('reviewText', axis =1,inplace=True)\n",
    "df1.count()\n",
    "\n",
    "df2 = pd.DataFrame(columns=[\"overall\"],data=predictions+1)\n",
    "# df3=df2.copy()\n",
    "# df3[\"overall\"] = df3[\"overall\"].round().astype(int)\n",
    "# print(type(df3[\"overall\"]))\n",
    "result = pd.concat([df1, df2], axis=1, sort=False)\n",
    "\n",
    "result.head()\n",
    "# result[\"overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('result_submission.csv',encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "tokens_new = [lmtzr.lemmatize(token,pos='v') for token in review_data_train['Summary_Clean_reviewtext']]\n",
    "print(tokens_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = 0\n",
    "for ngram in nltk.ngrams(tokens_new, 2): # nltk has the ngrams function that returns a generator of n-grams given a tokenized sentence\n",
    "    print (ngram,)\n",
    "    tmp += 1\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_freq = nltk.FreqDist() # We initialized a frequency counter\n",
    "# FreqDist gives the frequency of words within a text\n",
    "for ngram in nltk.ngrams(tokens_new, 2):\n",
    "    ngram_freq[ngram] += 1\n",
    "ngram_freq.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    from nltk.corpus import RegexpTokenizer as regextoken\n",
    "    tokenizer = regextoken('\\w+') \n",
    "    stop = stopwords.words('english')\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    sentence_tokens = [tokenizer.tokenize(sentence) for sentence in sentences] # list of lists\n",
    "    tokens = [] # initialising variable\n",
    "    for sentence in sentence_tokens:\n",
    "        sent = []\n",
    "        for word in sentence:\n",
    "            if word.lower() not in stop:\n",
    "                sent.append(word.lower())\n",
    "        tokens.append(sent)\n",
    "    \n",
    "    ## THE SAME FOR LOOP CAN BE WRITTEN AS FOLLOWS\n",
    "    ## tokens = [[word.lower() for word in sent if word not in stop] for sent in sentence_tokens]\n",
    "    \n",
    "    tokens = [[lmtzr.lemmatize(word) for word in sent] for sent in tokens]\n",
    "    return tokens\n",
    "\n",
    "def process_ngrams(input_sentence_tokens):\n",
    "    ngram_list = []\n",
    "    for sentence in input_sentence_tokens:\n",
    "        ngram_sent = nltk.ngrams(sentence, 2)\n",
    "        ngram_list = ngram_list + list(ngram_sent)\n",
    "    return ngram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reviews = review_data_train[\"Summary_Clean_reviewtext\"].size\n",
    "num_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reviews_test = review_data_test[\"Summary_Clean_reviewtext\"].size\n",
    "num_reviews_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews =[]\n",
    "for i in range(0, num_reviews):\n",
    "    reviews.append(process_text(review_data_train[\"Summary_Clean_reviewtext\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(reviews[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_test =[]\n",
    "for i in range(0, num_reviews_test):\n",
    "    reviews_test.append(process_text(review_data_test[\"Summary_Clean_reviewtext\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(reviews_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words( raw_review ):\n",
    "        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", str(raw_review))\n",
    "    words = letters_only.lower().split()                             \n",
    "    stops = set(stopwords.words(\"english\"))                   \n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    return( \" \".join( meaningful_words )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "very_clean_review =[]\n",
    "for i in range(0, num_reviews):\n",
    "    very_clean_review.append(review_to_words(reviews[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "very_clean_review_test =[]\n",
    "for i in range(0, num_reviews_test):\n",
    "    very_clean_review_test.append(review_to_words(reviews_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(very_clean_review[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(very_clean_review_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence_tokens = process_text(str(review_data_train['Summary_Clean_reviewtext']))\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_ngrams = process_ngrams(sentence_tokens)\n",
    "print(string_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_freq = nltk.FreqDist() #WE INITIALIZED A FREQUENCY COUNTER\n",
    "for ngram in string_ngrams:\n",
    "    ngram_freq[ngram] += 1\n",
    "print(ngram_freq.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_reviews = review_data_train[\"Summary_Clean_reviewtext\"].size\n",
    "\n",
    "print(num_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words( raw_review ):\n",
    "        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", str(raw_review))\n",
    "    words = letters_only.lower().split()                             \n",
    "    stops = set(stopwords.words(\"english\"))                   \n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    return( \" \".join( meaningful_words )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_reviews = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list \n",
    "for i in range( 0, num_reviews ):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean reviews\n",
    "    clean_reviews.append(review_to_words( review_data_train[\"Summary_Clean_reviewtext\"][i] ) )\n",
    "\n",
    "#print(clean_reviews)\n",
    "\n",
    "print(clean_reviews[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_reviews_test = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list \n",
    "for i in range( 0, num_reviews_test ):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean reviews\n",
    "    clean_reviews_test.append(review_to_words( review_data_test[\"Summary_Clean_reviewtext\"][i] ) )\n",
    "\n",
    "#print(clean_reviews)\n",
    "\n",
    "print(clean_reviews_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sklearn_tfidf = TfidfVectorizer(norm='l2',min_df=0, smooth_idf=True, tokenizer=None)\n",
    "\n",
    "sklearn_representation = sklearn_tfidf.fit_transform(very_clean_review)\n",
    "print(sklearn_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sklearn_tfidf = TfidfVectorizer(norm='l2',min_df=0, smooth_idf=True, tokenizer=None)\n",
    "\n",
    "sklearn_representation = sklearn_tfidf.fit_transform(very_clean_review_test)\n",
    "print(sklearn_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features_test = sklearn_representation.toarray()\n",
    "print(data_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy arrays are easy to work with, so convert the result to an array\n",
    "data_features = sklearn_representation.toarray()\n",
    "print(data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features=pd.DataFrame(data_features)\n",
    "print(data_features)\n",
    "print(data_features.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features.loc[\"0\":\"1\",\"5760\":\"5762\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features[\"overall\"]=review_data_train[\"overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = data_features.sample(frac=0.2)\n",
    "print(data_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting data into train & test\n",
    "\n",
    "train,test = train_test_split(data_features, test_size = 0.3) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [col for col in data_features.columns if col not in [\"overall\"]]\n",
    "\n",
    "train.x= train[cols]\n",
    "train.y=train[\"overall\"]\n",
    "\n",
    "test.x=test[cols]\n",
    "test.y=test[\"overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(train.x, train.y).predict(test.x)\n",
    "confusion_matrix(test.y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_nb = metrics.accuracy_score(test.y, y_pred)\n",
    "print(accuracy_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg=LogisticRegression()\n",
    "logreg.fit(train.x,train.y)\n",
    "y_pred=logreg.predict(test.x)\n",
    "\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set:',logreg.score(test.x, test.y))\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test.y, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_nb = metrics.accuracy_score(test.y, y_pred)\n",
    "print(accuracy_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=logreg.predict(data_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.y.to_csv(\"./train_split_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.x.to_csv(\"./train_split_2_x.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing tf-idf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfVectorizer(ngram_range=(1,1), stop_words='english')\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(review_data_train['Summary_Clean_reviewtext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dense_mat = X_train_tfidf.todense()\n",
    "\n",
    "type(Dense_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfidf_transformer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets consider TF-idf\n",
    "Dense_mat = X_train_tfidf.todense()\n",
    "A = pd.DataFrame(Dense_mat, columns=tfidf_transformer.get_feature_names())\n",
    "A.shape\n",
    "A.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(Dense_mat, review_data_train['Summary_Clean_reviewtext'],test_size=0.3,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_train = MultinomialNB()\n",
    "\n",
    "clf_train.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf_train.predict(X_test)\n",
    "\n",
    "prediction = confusion_matrix(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, accuracy_score\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "\n",
    "\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dimension = train_tfidf_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl_X = train_tfidf_X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl_y = train_tfidf_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(train_tfidf_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units = 32, input_dim=9000, activation='relu', kernel_initializer='uniform'))\n",
    "model.add(Dense(units = 32, activation='relu', kernel_initializer='uniform'))\n",
    "model.add(Dense(output_dim=6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(np.array(train_tfidf_X), train_tfidf_y, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(train_tf, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, LSTM, Dropout, Embedding\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "from keras.preprocessing import sequence #To convert a variable length sentence into a prespecified length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.append(train_dl_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.append(train_dl_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model lstm\n",
    "embedding_vecor_length = 32\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(9000, embedding_vecor_length, input_length=9000))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(output_dim=5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(a,b, epochs=150, batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
